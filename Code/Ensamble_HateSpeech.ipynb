{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a8da0e-c91c-49e4-8bc9-8e519e1c670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-20 16:56:20--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8002036 (7.6M) [text/plain]\n",
      "Saving to: ‚Äòblp25_hatespeech_subtask_1A_train.tsv.4‚Äô\n",
      "\n",
      "blp25_hatespeech_su 100%[===================>]   7.63M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2025-08-20 16:56:20 (134 MB/s) - ‚Äòblp25_hatespeech_subtask_1A_train.tsv.4‚Äô saved [8002036/8002036]\n",
      "\n",
      "--2025-08-20 16:56:21--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 585339 (572K) [text/plain]\n",
      "Saving to: ‚Äòblp25_hatespeech_subtask_1A_dev.tsv.4‚Äô\n",
      "\n",
      "blp25_hatespeech_su 100%[===================>] 571.62K  2.72MB/s    in 0.2s    \n",
      "\n",
      "2025-08-20 16:56:21 (2.72 MB/s) - ‚Äòblp25_hatespeech_subtask_1A_dev.tsv.4‚Äô saved [585339/585339]\n",
      "\n",
      "--2025-08-20 16:56:21--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev_test.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 548258 (535K) [text/plain]\n",
      "Saving to: ‚Äòblp25_hatespeech_subtask_1A_dev_test.tsv.3‚Äô\n",
      "\n",
      "blp25_hatespeech_su 100%[===================>] 535.41K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-08-20 16:56:21 (18.1 MB/s) - ‚Äòblp25_hatespeech_subtask_1A_dev_test.tsv.3‚Äô saved [548258/548258]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_train.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8140d990-3e8f-49a9-a3ad-ed59bf720a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91188796-0358-4261-b304-1f5fb8c0fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[INFO|training_args.py:2183] 2025-08-20 17:35:26,226 >> PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Ensamble_HateSpeech/\",\n",
    "    \n",
    "    # Your core changes\n",
    "    num_train_epochs=5,  # Increased from 1 to 3. You can experiment with 4 or 5.\n",
    "    \n",
    "    # Important adjustments for finding the best model\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,  # This will load the best checkpoint when training is done\n",
    "    \n",
    "    # Other common parameters\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=2,           # Saves disk space by only keeping the best 2 checkpoints\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "max_train_samples = None\n",
    "max_eval_samples=None\n",
    "max_predict_samples=None\n",
    "max_seq_length = 512\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e260bde0-8572-4027-9f8f-78656e272c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 17:35:26 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0 distributed training: True, 16-bits training: False\n",
      "08/20/2025 17:35:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./Ensamble_HateSpeech/runs/Aug20_17-35-26_node414.cluster,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./Ensamble_HateSpeech/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./Ensamble_HateSpeech/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec688fd5-c0a5-48f7-9a76-0c00dc126916",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'blp25_hatespeech_subtask_1A_train.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_dev_test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d4d40db-6514-4d29-b1ee-d87e735a8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mDeBERTa‚Äëv3‚Äëbase'\n",
    "\n",
    "#model_name = \"csebuetnlp/banglabert\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e2b92b4-1b42-4726-b280-2f670a79d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 17:35:27 - INFO - __main__ - loading a local file for train\n",
      "08/20/2025 17:35:27 - INFO - __main__ - loading a local file for validation\n",
      "08/20/2025 17:35:27 - INFO - __main__ - loading a local file for test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label'],\n",
       "    num_rows: 35522\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "train_df = pd.read_csv(train_file, sep='\\t')\n",
    "# print(train_df['label'])\n",
    "train_df['label'] = train_df['label'].map(l2id).fillna(0).astype(int)\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "validation_df = pd.read_csv(validation_file, sep='\\t')\n",
    "validation_df['label'] = validation_df['label'].map(l2id).fillna(0).astype(int)\n",
    "validation_df = Dataset.from_pandas(validation_df)\n",
    "test_df = pd.read_csv(test_file, sep='\\t')\n",
    "#test_df['label'] = test_df['label'].map(l2id)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "data_files = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
    "for key in data_files.keys():\n",
    "    logger.info(f\"loading a local file for {key}\")\n",
    "raw_datasets = DatasetDict(\n",
    "    {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
    ")\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39ce3048-859c-4dbf-8088-a2c5877bf98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               text  label\n",
      "0  147963  ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶∞‡ßç‡¶°‡¶æ‡¶∞ ‡¶ó‡¶æ‡¶∞‡ßç‡¶° ‡¶¶‡ßá‡¶∞‡¶ï‡ßá ‡¶è‡¶≠‡¶æ‡¶¨‡ßá ‡¶™‡¶æ‡¶π‡¶æ‡¶∞‡¶æ ‡¶¶‡¶ø‡¶§‡ßá ‡¶π...      0\n",
      "1  214275  ‡¶õ‡ßã‡¶ü‡¶¨‡ßá‡¶≤‡¶æ‡¶Ø‡¶º ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ï‡¶∑‡ßç‡¶ü ‡¶ï‡¶∞‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ó‡¶æ‡¶≤‡¶æ‡¶ó‡¶æ‡¶≤‡¶ø ‡¶∂‡¶ø‡¶ñ‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ...      0\n",
      "2  849172          ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶è ‡¶®‡¶ø‡¶ú‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶¶‡ßÅ‡¶∞ ‡¶¨‡¶æ‡¶®‡¶æ‡¶á‡ßü‡¶æ ‡¶´‡ßá‡¶≤‡¶õ‡ßá‡¶® ‡¶∞‡ßá      5\n",
      "3  821985  ‡¶ö‡¶ø‡¶® ‡¶≠‡¶æ‡¶∞‡¶§ ‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ ‡¶è‡¶á ‡¶§‡¶ø‡¶® ‡¶¶‡ßá‡¶∂ ‡¶è‡¶ï ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶ï‡ßá ‡¶∂‡¶æ...      0\n",
      "4  477288  ‡¶è‡¶ü‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶ï‡ßá ‡¶ï‡¶∞‡¶¨‡ßá‡¶Ø‡ßá ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá ‡¶∏‡ßá‡¶á ‡¶§‡ßã ‡¶π‡¶≤‡ßã ‡¶è‡¶á ...      5\n",
      "5  933728    ‡¶§‡ßÅ‡¶∞‡¶æ ‡¶ï‡¶ø‡¶∏‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡ßÅ‡¶≠‡¶æ‡¶á ‡¶Ø‡¶æ‡¶¨‡¶ø ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶¶‡ßá‡¶∂‡ßá ‡¶ï‡ßá‡¶® ‡¶ú‡¶æ‡¶∏ ‡¶®‡¶æ      0\n",
      "6  398351  ‡¶¶‡ßá‡¶∂ ‡¶¨‡¶ø‡¶≠‡¶æ‡¶ó‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ ‡¶™‡¶æ‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶®‡ßá ‡ß®‡ß´ ‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂ ‡¶∏...      0\n",
      "7  786609  ‡¶á‡¶∞‡¶æ‡¶® ‡¶™‡¶æ‡¶∞‡¶Æ‡¶æ‡¶£‡¶¨‡¶ø‡¶ï ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡¶¨‡ßá ‡¶¨‡¶æ‡¶®‡¶æ‡¶¨‡ßá ‡¶¨‡¶≤‡¶§‡ßá ‡¶¨‡¶≤‡¶§‡ßá ‡¶¨‡¶ø...      5\n",
      "8  917115                                  ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶è‡¶á ‡¶â‡ßé‡¶∏‡¶¨ ‡¶ï‡ßá‡¶®‡ßã      0\n",
      "9  415453  ‡¶á‡¶Æ‡¶∞‡¶æ‡¶® ‡¶õ‡¶æ‡ßú‡¶æ ‡¶™‡¶æ‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶® ‡¶ï‡¶ñ‡¶®‡ßã‡¶á ‡¶ò‡ßÅ‡¶∞‡ßá ‡¶¶‡¶æ‡ßú‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ‡¶®...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert first 10 rows of the \"train\" split into a pandas DataFrame\n",
    "df = raw_datasets[\"train\"].to_pandas()\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a80c7dd6-bb46-48c3-b866-dc041c520491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 17:35:28 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0 distributed training: True, 16-bits training: False\n",
      "08/20/2025 17:35:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./Ensamble_HateSpeech/runs/Aug20_17-35-26_node414.cluster,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./Ensamble_HateSpeech/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./Ensamble_HateSpeech/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "614864de-b344-4426-84ee-d816a4f779a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 4, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "print(label_list)\n",
    "label_list.sort()  # sort the labels for determine\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8fd1cfd-4072-4e5b-a9ed-8d532c65e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:699] 2025-08-20 17:35:29,409 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-20 17:35:29,409 >> Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:699] 2025-08-20 17:35:29,456 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-20 17:35:29,457 >> Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-20 17:35:29,458 >> loading file spm.model from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/spm.model\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-20 17:35:29,458 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-20 17:35:29,458 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-20 17:35:29,458 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-20 17:35:29,458 >> loading file tokenizer_config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-20 17:35:29,458 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:699] 2025-08-20 17:35:29,459 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-20 17:35:29,460 >> Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:699] 2025-08-20 17:35:29,675 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-20 17:35:29,676 >> Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:3982] 2025-08-20 17:35:29,890 >> loading weights file pytorch_model.bin from cache at /users/t/p/tprama/.cache/huggingface/hub/models--microsoft--deberta-v3-base/snapshots/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/pytorch_model.bin\n",
      "[INFO|safetensors_conversion.py:61] 2025-08-20 17:35:29,939 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-08-20 17:35:30,064 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:4960] 2025-08-20 17:35:30,662 >> Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4972] 2025-08-20 17:35:30,663 >> Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Define your model name and number of labels\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "num_labels = 6 # Based on your l2id dictionary: None, Religious, Sexism, etc.\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "# Load the model FOR SEQUENCE CLASSIFICATION\n",
    "# This is the key change!\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True, # Important when adding a new head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "334539e7-8910-4962-9940-e508d7613069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa81c15dc634cb3bc3883bd79881eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/35522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8274825206614f81aca06a029fb29257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04df8d1da2214c3eb89ebce9a8d163f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "sentence1_key= non_label_column_names[1]\n",
    "\n",
    "# Padding strategy\n",
    "padding = \"max_length\"\n",
    "\n",
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
    "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",)\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "if 128 > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({128}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\")\n",
    "max_seq_length = min(128, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],))\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a405735-dd8f-4b76-a1f9-79f6412f924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"train\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a train dataset\")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "if max_train_samples is not None:\n",
    "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c607bb2-3fce-48db-89b2-8e5f405e3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a validation dataset\")\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "if max_eval_samples is not None:\n",
    "    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85f18dbf-98f6-453f-9842-b31d6df35ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a test dataset\")\n",
    "predict_dataset = raw_datasets[\"test\"]\n",
    "if max_predict_samples is not None:\n",
    "    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n",
    "    predict_dataset = predict_dataset.select(range(max_predict_samples_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e9c8e8e-7d61-4e32-873e-1d9cf0f7cae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 17:35:33 - INFO - __main__ - Sample 7296 of the training set: {'id': 660, 'text': '‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶¶‡¶æ‡ßü‡¶ø‡¶§‡ßç‡¶¨‡¶™‡ßç‡¶∞‡¶æ‡¶™‡ßç‡¶§ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ ‡¶∏‡¶ï‡¶≤ ‡¶¨‡¶æ‡¶£‡¶ø‡¶ú‡ßç‡¶Ø‡¶ø‡¶ï ‡¶≠‡¶¨‡¶® ‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶¨‡¶õ‡¶∞‡ßá ‡¶Ö‡¶®‡ßç‡¶§‡¶§ ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¶‡¶∞‡ßç‡¶∂‡¶® ‡¶ï‡¶∞‡ßá ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡¶æ‡¶£‡¶ø‡¶ú‡ßç‡¶Ø‡¶ø‡¶ï ‡¶≠‡¶¨‡¶®‡ßá ‡¶®‡¶ø‡¶∞‡¶æ‡¶™‡¶§‡ßç‡¶§‡¶æ‡¶∞ ‡¶∏‡¶ï‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ ‡¶•‡¶æ‡¶ï‡¶æ ‡¶â‡¶ö‡¶ø‡¶§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶è‡¶á ‡¶∏‡¶¨ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡ßá‡¶∞ ‡¶¶‡ßá‡¶∂‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ‡¶®‡¶æ ‡¶è‡¶á ‡¶¶‡ßá‡¶∂‡ßá ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶ï‡ßã‡¶® ‡¶¶‡¶æ‡¶Æ ‡¶®‡ßá‡¶á ‡¶§‡¶æ‡¶á ‡¶ï‡ßá‡¶â ‡¶ï‡ßã‡¶®‡¶ï‡¶ø‡¶õ‡ßÅ‡¶∞ ‡¶§‡¶¶‡¶æ‡¶∞‡¶ï‡¶ø‡¶∞‡¶ì ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ', 'label': 3, 'input_ids': [1, 119491, 84555, 67300, 105927, 53401, 84555, 507, 103135, 42055, 107186, 56829, 82319, 70385, 69119, 91091, 70385, 84555, 42055, 91091, 70385, 82319, 119491, 123544, 115329, 70385, 123207, 42055, 119491, 67300, 84144, 507, 69119, 42055, 123974, 56829, 116962, 70385, 123528, 56829, 67300, 507, 123047, 69119, 57264, 507, 122570, 101628, 84144, 113711, 67300, 53401, 507, 69119, 122838, 84555, 53401, 507, 123179, 57264, 70385, 82319, 82319, 507, 122630, 67300, 69119, 105927, 507, 91091, 84555, 56829, 103135, 84555, 70385, 122534, 57264, 507, 67300, 84555, 53401, 507, 84555, 56829, 91091, 113711, 84555, 70385, 110358, 507, 91091, 70385, 84555, 103135, 42055, 57264, 507, 67300, 84555, 69119, 53401, 57264, 507, 122630, 67300, 110358, 42055, 507, 69119, 42055, 123974, 56829, 116962, 70385, 123528, 56829, 67300, 507, 123047, 69119, 57264, 53401, 507, 57264, 56829, 84555, 42055, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
      "08/20/2025 17:35:33 - INFO - __main__ - Sample 1639 of the training set: {'id': 463531, 'text': '‡¶ï‡ßá ‡¶ï‡ßá ‡¶ü‡¶æ ‡¶∞‡ßã‡¶ú ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßã ‡¶è‡¶¨‡¶Ç ‡¶™‡¶æ‡¶Å‡¶ö ‡¶ì‡ßü‡¶æ‡¶ï‡ßç‡¶§ ‡¶®‡¶æ‡¶Æ‡¶æ‡¶ú ‡¶Ü‡¶¶‡¶æ‡ßü ‡¶ï‡¶∞‡¶¨', 'label': 0, 'input_ids': [1, 507, 67300, 53401, 507, 67300, 53401, 507, 110358, 42055, 507, 84555, 113711, 116962, 507, 84555, 42055, 123166, 69119, 113711, 507, 122630, 69119, 123544, 507, 91091, 42055, 125131, 122792, 507, 123494, 107186, 42055, 67300, 70385, 82319, 507, 57264, 42055, 85688, 42055, 116962, 507, 122498, 103135, 42055, 107186, 507, 67300, 84555, 69119, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "08/20/2025 17:35:33 - INFO - __main__ - Sample 18024 of the training set: {'id': 156766, 'text': '‡¶â‡¶®‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â‡¶ö‡ßç‡¶õ‡ßá‡¶¶ ‡¶ï‡¶∞‡¶æ ‡¶â‡¶ö‡¶ø‡¶§ ‡¶π‡¶¨‡ßá‡¶®‡¶æ ‡¶â‡¶®‡¶æ‡¶∞‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶æ‡¶§‡¶ø ‡¶∏‡¶§‡ßç‡¶§‡¶æ‡¶∞ ‡¶Ö‡¶Ç‡¶∂', 'label': 0, 'input_ids': [1, 507, 123826, 57264, 42055, 103135, 53401, 84555, 507, 123826, 122792, 70385, 122838, 53401, 103135, 507, 67300, 84555, 42055, 507, 123826, 122792, 56829, 82319, 507, 119286, 69119, 53401, 57264, 42055, 507, 123826, 57264, 105927, 42055, 507, 122498, 85688, 42055, 103135, 53401, 84555, 507, 116962, 42055, 82319, 56829, 119491, 82319, 70385, 82319, 105927, 507, 123179, 123544, 122534, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    }
   ],
   "source": [
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c5c24ba-96b7-44fe-961d-a1af3b482613",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74670641-e992-41f5-9772-4393f0b092fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ca2b223-7fec-4add-b71f-7c1cf487bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b67bef0-077b-4240-8cdc-6143ebf8a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(\"id\")\n",
    "eval_dataset = eval_dataset.remove_columns(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f9cafd7-ef38-4ba3-937e-6892eacf0597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1908056/1049306949.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef9555ac-2599-412e-9c0a-660814b9a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-20 17:35:35,246 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2405] 2025-08-20 17:35:35,252 >> ***** Running training *****\n",
      "[INFO|trainer.py:2406] 2025-08-20 17:35:35,252 >>   Num examples = 35,522\n",
      "[INFO|trainer.py:2407] 2025-08-20 17:35:35,252 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2408] 2025-08-20 17:35:35,252 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2411] 2025-08-20 17:35:35,252 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2412] 2025-08-20 17:35:35,253 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2413] 2025-08-20 17:35:35,253 >>   Total optimization steps = 11,105\n",
      "[INFO|trainer.py:2414] 2025-08-20 17:35:35,253 >>   Number of trainable parameters = 184,426,758\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11105' max='11105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11105/11105 2:41:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>0.846010</td>\n",
       "      <td>0.685510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.809374</td>\n",
       "      <td>0.696656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.791700</td>\n",
       "      <td>0.787678</td>\n",
       "      <td>0.704220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.752900</td>\n",
       "      <td>0.761758</td>\n",
       "      <td>0.704618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.763972</td>\n",
       "      <td>0.709793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-20 18:07:20,310 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 18:07:20,313 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 18:07:20,314 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 18:07:20,314 >>   Batch size = 16\n",
      "[INFO|trainer.py:3942] 2025-08-20 18:07:56,523 >> Saving model checkpoint to ./Ensamble_HateSpeech/checkpoint-2221\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 18:07:56,525 >> Configuration saved in ./Ensamble_HateSpeech/checkpoint-2221/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 18:07:56,700 >> Model weights saved in ./Ensamble_HateSpeech/checkpoint-2221/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 18:07:56,702 >> tokenizer config file saved in ./Ensamble_HateSpeech/checkpoint-2221/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 18:07:56,703 >> Special tokens file saved in ./Ensamble_HateSpeech/checkpoint-2221/special_tokens_map.json\n",
      "[INFO|trainer.py:928] 2025-08-20 18:39:39,527 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 18:39:39,529 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 18:39:39,530 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 18:39:39,531 >>   Batch size = 16\n",
      "[INFO|trainer.py:3942] 2025-08-20 18:40:15,173 >> Saving model checkpoint to ./Ensamble_HateSpeech/checkpoint-4442\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 18:40:15,174 >> Configuration saved in ./Ensamble_HateSpeech/checkpoint-4442/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 18:40:15,354 >> Model weights saved in ./Ensamble_HateSpeech/checkpoint-4442/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 18:40:15,356 >> tokenizer config file saved in ./Ensamble_HateSpeech/checkpoint-4442/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 18:40:15,357 >> Special tokens file saved in ./Ensamble_HateSpeech/checkpoint-4442/special_tokens_map.json\n",
      "[INFO|trainer.py:928] 2025-08-20 19:12:00,489 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 19:12:00,492 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 19:12:00,493 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 19:12:00,493 >>   Batch size = 16\n",
      "[INFO|trainer.py:3942] 2025-08-20 19:12:36,208 >> Saving model checkpoint to ./Ensamble_HateSpeech/checkpoint-6663\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 19:12:36,210 >> Configuration saved in ./Ensamble_HateSpeech/checkpoint-6663/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 19:12:36,384 >> Model weights saved in ./Ensamble_HateSpeech/checkpoint-6663/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 19:12:36,386 >> tokenizer config file saved in ./Ensamble_HateSpeech/checkpoint-6663/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 19:12:36,387 >> Special tokens file saved in ./Ensamble_HateSpeech/checkpoint-6663/special_tokens_map.json\n",
      "[INFO|trainer.py:4034] 2025-08-20 19:12:36,994 >> Deleting older checkpoint [Ensamble_HateSpeech/checkpoint-2221] due to args.save_total_limit\n",
      "[INFO|trainer.py:928] 2025-08-20 19:44:21,190 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 19:44:21,192 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 19:44:21,193 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 19:44:21,193 >>   Batch size = 16\n",
      "[INFO|trainer.py:3942] 2025-08-20 19:44:56,924 >> Saving model checkpoint to ./Ensamble_HateSpeech/checkpoint-8884\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 19:44:56,926 >> Configuration saved in ./Ensamble_HateSpeech/checkpoint-8884/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 19:44:57,128 >> Model weights saved in ./Ensamble_HateSpeech/checkpoint-8884/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 19:44:57,130 >> tokenizer config file saved in ./Ensamble_HateSpeech/checkpoint-8884/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 19:44:57,131 >> Special tokens file saved in ./Ensamble_HateSpeech/checkpoint-8884/special_tokens_map.json\n",
      "[INFO|trainer.py:4034] 2025-08-20 19:44:57,746 >> Deleting older checkpoint [Ensamble_HateSpeech/checkpoint-4442] due to args.save_total_limit\n",
      "[INFO|trainer.py:3942] 2025-08-20 20:16:45,896 >> Saving model checkpoint to ./Ensamble_HateSpeech/checkpoint-11105\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 20:16:45,898 >> Configuration saved in ./Ensamble_HateSpeech/checkpoint-11105/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 20:16:46,073 >> Model weights saved in ./Ensamble_HateSpeech/checkpoint-11105/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 20:16:46,075 >> tokenizer config file saved in ./Ensamble_HateSpeech/checkpoint-11105/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 20:16:46,076 >> Special tokens file saved in ./Ensamble_HateSpeech/checkpoint-11105/special_tokens_map.json\n",
      "[INFO|trainer.py:4034] 2025-08-20 20:16:46,662 >> Deleting older checkpoint [Ensamble_HateSpeech/checkpoint-6663] due to args.save_total_limit\n",
      "[INFO|trainer.py:928] 2025-08-20 20:16:46,665 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 20:16:46,667 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 20:16:46,668 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 20:16:46,668 >>   Batch size = 16\n",
      "[INFO|trainer.py:3942] 2025-08-20 20:17:22,366 >> Saving model checkpoint to ./Ensamble_HateSpeech/checkpoint-11105\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 20:17:22,368 >> Configuration saved in ./Ensamble_HateSpeech/checkpoint-11105/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 20:17:22,555 >> Model weights saved in ./Ensamble_HateSpeech/checkpoint-11105/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 20:17:22,557 >> tokenizer config file saved in ./Ensamble_HateSpeech/checkpoint-11105/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 20:17:22,557 >> Special tokens file saved in ./Ensamble_HateSpeech/checkpoint-11105/special_tokens_map.json\n",
      "[INFO|trainer.py:2657] 2025-08-20 20:17:23,174 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2895] 2025-08-20 20:17:23,174 >> Loading best model from ./Ensamble_HateSpeech/checkpoint-8884 (score: 0.7617575526237488).\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = (\n",
    "    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "587c4504-2901-4d2c-917b-4f1964d70171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3942] 2025-08-20 20:31:36,977 >> Saving model checkpoint to ./Ensamble_HateSpeech/\n",
      "[INFO|configuration_utils.py:423] 2025-08-20 20:31:36,979 >> Configuration saved in ./Ensamble_HateSpeech/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-20 20:31:37,143 >> Model weights saved in ./Ensamble_HateSpeech/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-20 20:31:37,144 >> tokenizer config file saved in ./Ensamble_HateSpeech/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-20 20:31:37,144 >> Special tokens file saved in ./Ensamble_HateSpeech/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               = 10881030GF\n",
      "  train_loss               =     0.8249\n",
      "  train_runtime            = 2:41:48.34\n",
      "  train_samples            =      35522\n",
      "  train_samples_per_second =     18.295\n",
      "  train_steps_per_second   =      1.144\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7725a83-7b8a-4c82-b2a9-684fa46eecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 20:31:49 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-20 20:31:49,043 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 20:31:49,045 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 20:31:49,045 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 20:31:49,046 >>   Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =     0.7046\n",
      "  eval_loss               =     0.7618\n",
      "  eval_runtime            = 0:00:36.31\n",
      "  eval_samples            =       2512\n",
      "  eval_samples_per_second =     69.175\n",
      "  eval_steps_per_second   =      4.323\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "max_eval_samples = (\n",
    "    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",
    ")\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2047907a-2fc4-4259-9254-feb0729cd700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 20:32:44 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-20 20:32:44,095 >> The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-20 20:32:44,096 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4260] 2025-08-20 20:32:44,096 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-20 20:32:44,096 >>   Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 20:33:19 - INFO - __main__ - ***** Predict results *****\n"
     ]
    }
   ],
   "source": [
    "id2l = {v: k for k, v in l2id.items()}\n",
    "logger.info(\"*** Predict ***\")\n",
    "#predict_dataset = predict_dataset.remove_columns(\"label\")\n",
    "ids = predict_dataset['id']\n",
    "predict_dataset = predict_dataset.remove_columns(\"id\")\n",
    "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "output_predict_file = os.path.join(training_args.output_dir, f\"subtask_1A.tsv\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_predict_file, \"w\") as writer:\n",
    "        logger.info(f\"***** Predict results *****\")\n",
    "        writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "        for index, item in enumerate(predictions):\n",
    "            item = label_list[item]\n",
    "            item = id2l[item]\n",
    "            writer.write(f\"{ids[index]}\\t{item}\\t{model_name}\\n\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78240aa0-ac3d-4f5f-93bd-8db9c5b99ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:449] 2025-08-20 20:33:34,129 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7046178579330444}]}\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\n",
    "trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87dc8655-785c-43c9-aef5-bd49ed10f83b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument `a` is not recognized as numeric. Support for input that cannot be coerced to a numeric array was deprecated in SciPy 1.9.0 and removed in SciPy 1.11.0. Please consider `np.unique`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1908056/2329708400.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"ens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Majority vote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"final_pred\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"distil\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Save new ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"final_pred\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_ensemble.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    656\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNaN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotest_fun_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotest_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_add_reduced_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         raise ValueError(\n\u001b[1;32m    377\u001b[0m             \u001b[0;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# remove the requested axis, and add the new ones on the end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_remove_sentinel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaired\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_too_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNaN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotest_fun_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.12/site-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, axis, nan_policy, keepdims)\u001b[0m\n\u001b[1;32m    555\u001b[0m         message = (\"Argument `a` is not recognized as numeric. \"\n\u001b[1;32m    556\u001b[0m                    \u001b[0;34m\"Support for input that cannot be coerced to a numeric \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                    \u001b[0;34m\"array was deprecated in SciPy 1.9.0 and removed in SciPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                    \u001b[0;34m\"1.11.0. Please consider `np.unique`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mNaN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `a` is not recognized as numeric. Support for input that cannot be coerced to a numeric array was deprecated in SciPy 1.9.0 and removed in SciPy 1.11.0. Please consider `np.unique`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load predictions\n",
    "bert = pd.read_csv(\"./BanglaBERT_HateSpeech/subtask_1A.tsv\", sep=\"\\t\")\n",
    "distil = pd.read_csv(\"./distilBERT_m/subtask_1A.tsv\", sep=\"\\t\")\n",
    "ens = pd.read_csv(\"./Ensamble_HateSpeech//subtask_1A.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Assume each file has [\"id\",\"label\"]\n",
    "df = pd.DataFrame({\n",
    "    \"id\": bert[\"id\"],\n",
    "    \"bert\": bert[\"label\"],\n",
    "    \"distil\": distil[\"label\"],\n",
    "    \"ens\": ens[\"label\"]\n",
    "})\n",
    "\n",
    "# Majority vote\n",
    "df[\"final_pred\"], _ = mode(df[[\"bert\",\"distil\",\"ens\"]].values, axis=1)\n",
    "\n",
    "# Save new ensemble\n",
    "df[[\"id\",\"final_pred\"]].to_csv(\"final_ensemble.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Saved to final_ensemble.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a1f3a5a-6f93-48ce-b681-a46cfb658083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to final_ensemble.tsv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame({\n",
    "    \"id\": bert[\"id\"],\n",
    "    \"bert\": bert[\"label\"],\n",
    "    \"distil\": distil[\"label\"],\n",
    "    \"ens\": ens[\"label\"]\n",
    "})\n",
    "\n",
    "# majority vote using pandas mode (works with strings)\n",
    "df[\"final_pred\"] = df[[\"bert\",\"distil\",\"ens\"]].mode(axis=1)[0]\n",
    "\n",
    "df[[\"id\",\"final_pred\"]].to_csv(\"final_ensemble.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Saved to final_ensemble.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d64b29dd-ca1e-48c4-8900-08f6e3932d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert</th>\n",
       "      <th>distil</th>\n",
       "      <th>ens</th>\n",
       "      <th>final_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     bert   distil      ens final_pred\n",
       "0     879187      NaN      NaN      NaN        NaN\n",
       "1     316919  Profane  Profane  Profane    Profane\n",
       "2     916242      NaN      NaN      NaN        NaN\n",
       "3     786824      NaN      NaN      NaN        NaN\n",
       "4      47284      NaN      NaN      NaN        NaN\n",
       "...      ...      ...      ...      ...        ...\n",
       "2507  776466      NaN      NaN      NaN        NaN\n",
       "2508  849227  Profane  Profane  Profane    Profane\n",
       "2509  532697      NaN  Profane  Profane    Profane\n",
       "2510  861411      NaN      NaN      NaN        NaN\n",
       "2511  743242      NaN      NaN      NaN        NaN\n",
       "\n",
       "[2512 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "06ea738f-2637-4bf9-a462-07e58b57e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model']='Ensemble-based Transfromer model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb8093be-6f71-41b0-8faa-417cd1c710fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert</th>\n",
       "      <th>distil</th>\n",
       "      <th>ens</th>\n",
       "      <th>final_pred</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     bert   distil      ens final_pred  \\\n",
       "0     879187      NaN      NaN      NaN        NaN   \n",
       "1     316919  Profane  Profane  Profane    Profane   \n",
       "2     916242      NaN      NaN      NaN        NaN   \n",
       "3     786824      NaN      NaN      NaN        NaN   \n",
       "4      47284      NaN      NaN      NaN        NaN   \n",
       "...      ...      ...      ...      ...        ...   \n",
       "2507  776466      NaN      NaN      NaN        NaN   \n",
       "2508  849227  Profane  Profane  Profane    Profane   \n",
       "2509  532697      NaN  Profane  Profane    Profane   \n",
       "2510  861411      NaN      NaN      NaN        NaN   \n",
       "2511  743242      NaN      NaN      NaN        NaN   \n",
       "\n",
       "                                 model  \n",
       "0     Ensemble-based Transfromer model  \n",
       "1     Ensemble-based Transfromer model  \n",
       "2     Ensemble-based Transfromer model  \n",
       "3     Ensemble-based Transfromer model  \n",
       "4     Ensemble-based Transfromer model  \n",
       "...                                ...  \n",
       "2507  Ensemble-based Transfromer model  \n",
       "2508  Ensemble-based Transfromer model  \n",
       "2509  Ensemble-based Transfromer model  \n",
       "2510  Ensemble-based Transfromer model  \n",
       "2511  Ensemble-based Transfromer model  \n",
       "\n",
       "[2512 rows x 6 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a17380b-0351-456a-a8bf-23dd535ab692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert</th>\n",
       "      <th>distil</th>\n",
       "      <th>ens</th>\n",
       "      <th>final_pred</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     bert   distil      ens final_pred  \\\n",
       "0     879187      NaN      NaN      NaN        NaN   \n",
       "1     316919  Profane  Profane  Profane    Profane   \n",
       "2     916242      NaN      NaN      NaN        NaN   \n",
       "3     786824      NaN      NaN      NaN        NaN   \n",
       "4      47284      NaN      NaN      NaN        NaN   \n",
       "...      ...      ...      ...      ...        ...   \n",
       "2507  776466      NaN      NaN      NaN        NaN   \n",
       "2508  849227  Profane  Profane  Profane    Profane   \n",
       "2509  532697      NaN  Profane  Profane    Profane   \n",
       "2510  861411      NaN      NaN      NaN        NaN   \n",
       "2511  743242      NaN      NaN      NaN        NaN   \n",
       "\n",
       "                                 model  \n",
       "0     Ensemble-based Transfromer model  \n",
       "1     Ensemble-based Transfromer model  \n",
       "2     Ensemble-based Transfromer model  \n",
       "3     Ensemble-based Transfromer model  \n",
       "4     Ensemble-based Transfromer model  \n",
       "...                                ...  \n",
       "2507  Ensemble-based Transfromer model  \n",
       "2508  Ensemble-based Transfromer model  \n",
       "2509  Ensemble-based Transfromer model  \n",
       "2510  Ensemble-based Transfromer model  \n",
       "2511  Ensemble-based Transfromer model  \n",
       "\n",
       "[2512 rows x 6 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bda1ca71-9d6a-4e59-b941-fdbff70920ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"final_pred\"].where(pd.notna(df[\"final_pred\"]), 'None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1d23483-747c-4c2e-bfad-f6f32b4192fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert</th>\n",
       "      <th>distil</th>\n",
       "      <th>ens</th>\n",
       "      <th>final_pred</th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     bert   distil      ens final_pred  \\\n",
       "0     879187      NaN      NaN      NaN       None   \n",
       "1     316919  Profane  Profane  Profane    Profane   \n",
       "2     916242      NaN      NaN      NaN       None   \n",
       "3     786824      NaN      NaN      NaN       None   \n",
       "4      47284      NaN      NaN      NaN       None   \n",
       "...      ...      ...      ...      ...        ...   \n",
       "2507  776466      NaN      NaN      NaN       None   \n",
       "2508  849227  Profane  Profane  Profane    Profane   \n",
       "2509  532697      NaN  Profane  Profane    Profane   \n",
       "2510  861411      NaN      NaN      NaN       None   \n",
       "2511  743242      NaN      NaN      NaN       None   \n",
       "\n",
       "                                 model    label  \n",
       "0     Ensemble-based Transfromer model     None  \n",
       "1     Ensemble-based Transfromer model  Profane  \n",
       "2     Ensemble-based Transfromer model     None  \n",
       "3     Ensemble-based Transfromer model     None  \n",
       "4     Ensemble-based Transfromer model     None  \n",
       "...                                ...      ...  \n",
       "2507  Ensemble-based Transfromer model     None  \n",
       "2508  Ensemble-based Transfromer model  Profane  \n",
       "2509  Ensemble-based Transfromer model  Profane  \n",
       "2510  Ensemble-based Transfromer model     None  \n",
       "2511  Ensemble-based Transfromer model     None  \n",
       "\n",
       "[2512 rows x 7 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c37e9876-6d72-4ce3-ab03-df0a3d9e4a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to final_ensemble.tsv\n"
     ]
    }
   ],
   "source": [
    "df[[\"id\",\"label\", \"model\"]].to_csv(\"final_ensemble.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Saved to final_ensemble.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d30e3fb1-599a-4004-95c8-8c5bdaf67099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert</th>\n",
       "      <th>distil</th>\n",
       "      <th>ens</th>\n",
       "      <th>final_pred</th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Profane</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>Profane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensemble-based Transfromer model</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     bert   distil      ens final_pred  \\\n",
       "0     879187      NaN      NaN      NaN       None   \n",
       "1     316919  Profane  Profane  Profane    Profane   \n",
       "2     916242      NaN      NaN      NaN       None   \n",
       "3     786824      NaN      NaN      NaN       None   \n",
       "4      47284      NaN      NaN      NaN       None   \n",
       "...      ...      ...      ...      ...        ...   \n",
       "2507  776466      NaN      NaN      NaN       None   \n",
       "2508  849227  Profane  Profane  Profane    Profane   \n",
       "2509  532697      NaN  Profane  Profane    Profane   \n",
       "2510  861411      NaN      NaN      NaN       None   \n",
       "2511  743242      NaN      NaN      NaN       None   \n",
       "\n",
       "                                 model    label  \n",
       "0     Ensemble-based Transfromer model     None  \n",
       "1     Ensemble-based Transfromer model  Profane  \n",
       "2     Ensemble-based Transfromer model     None  \n",
       "3     Ensemble-based Transfromer model     None  \n",
       "4     Ensemble-based Transfromer model     None  \n",
       "...                                ...      ...  \n",
       "2507  Ensemble-based Transfromer model     None  \n",
       "2508  Ensemble-based Transfromer model  Profane  \n",
       "2509  Ensemble-based Transfromer model  Profane  \n",
       "2510  Ensemble-based Transfromer model     None  \n",
       "2511  Ensemble-based Transfromer model     None  \n",
       "\n",
       "[2512 rows x 7 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e19d16-e813-41aa-98d4-ccbc0e7d2bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
