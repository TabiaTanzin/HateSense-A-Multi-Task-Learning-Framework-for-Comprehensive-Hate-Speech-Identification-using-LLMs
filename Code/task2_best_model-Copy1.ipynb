{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c1ddf2-a5df-4213-aea7-d8ea2a4895a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = 'blp25_hatespeech_subtask_1B_train.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1B_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1B_test.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882097f7-cdf8-42e9-b2e5-aa73f3b5b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# Load train/val/test DataFrames\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\")\n",
    "dev_df = pd.read_csv(validation_file , sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b50aef-bc27-426d-ba4b-6aa74eb78364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>toxic</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147963</td>\n",
       "      <td>ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214275</td>\n",
       "      <td>ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>849172</td>\n",
       "      <td>অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>821985</td>\n",
       "      <td>চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>477288</td>\n",
       "      <td>এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>790325</td>\n",
       "      <td>তইওয়ানের এত ক্ষমতা হয়নি যে এক টুকরো জায়গা নষ্ট...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>328377</td>\n",
       "      <td>চুরের ঘরের চুর হালা</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>69803</td>\n",
       "      <td>জাহাঙ্গীর বুদ্ধি নেই মাঠে মারা যাবে</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35520</th>\n",
       "      <td>419984</td>\n",
       "      <td>একটা ফেইল্ড এস্টেট এও সুষ্ঠু নির্বাচন হয় নেতার...</td>\n",
       "      <td>Organization</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35521</th>\n",
       "      <td>538747</td>\n",
       "      <td>ছেমরি হলো মাহফুজ রহমান কে ব্যাবহার করে কেরিয়ার...</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0      147963  ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...   \n",
       "1      214275  ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...   \n",
       "2      849172          অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে   \n",
       "3      821985  চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...   \n",
       "4      477288  এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...   \n",
       "...       ...                                                ...   \n",
       "35517  790325  তইওয়ানের এত ক্ষমতা হয়নি যে এক টুকরো জায়গা নষ্ট...   \n",
       "35518  328377                                চুরের ঘরের চুর হালা   \n",
       "35519   69803                জাহাঙ্গীর বুদ্ধি নেই মাঠে মারা যাবে   \n",
       "35520  419984  একটা ফেইল্ড এস্টেট এও সুষ্ঠু নির্বাচন হয় নেতার...   \n",
       "35521  538747  ছেমরি হলো মাহফুজ রহমান কে ব্যাবহার করে কেরিয়ার...   \n",
       "\n",
       "              label  toxic  label_id  \n",
       "0              None      0         0  \n",
       "1              None      0         0  \n",
       "2        Individual      1         4  \n",
       "3              None      0         0  \n",
       "4        Individual      1         4  \n",
       "...             ...    ...       ...  \n",
       "35517          None      0         0  \n",
       "35518          None      0         0  \n",
       "35519    Individual      1         4  \n",
       "35520  Organization      1         2  \n",
       "35521    Individual      1         4  \n",
       "\n",
       "[35522 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2id = {'None': 0, 'Society': 1, 'Organization': 2, 'Community': 3, 'Individual': 4}\n",
    "id2l = {v: k for k, v in l2id.items()}\n",
    "\n",
    "\n",
    "def clean_label(x):\n",
    "    # handle missing or NaN → \"None\"\n",
    "    if pd.isna(x) or x == 'None':\n",
    "        return 'None'\n",
    "    # already list-like e.g. ['Abusive']\n",
    "    if isinstance(x, list):\n",
    "        return x[0] if len(x) > 0 else 'None'\n",
    "    # string cases like \"[]\" or \"[Abusive]\" or \"[Political Hate]\"\n",
    "    x = x.strip(\"[]\").strip()\n",
    "    if x == \"\":\n",
    "        return 'None'\n",
    "    return x\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    # Ensure labels are proper lists\n",
    "    df[\"label\"] = df[\"label\"].apply(clean_label)\n",
    "    df[\"label\"] = df[\"label\"].fillna(\"None\")\n",
    "    # Now create binary label\n",
    "    df[\"toxic\"] = df[\"label\"].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "    df[\"label_id\"] = df[\"label\"].map(l2id)\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = process_df(train_df)\n",
    "dev_df  = process_df(dev_df)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56656f9a-fac6-4785-a6a0-2a67456a77a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  toxic\n",
      "0  ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...      0\n",
      "1  ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...      0\n",
      "2          অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে      1\n",
      "3  চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...      0\n",
      "4  এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...      1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the result\n",
    "print(train_df[['text', 'toxic']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8649a8-6d23-496e-b5a9-a974fed38517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>toxic</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166449</td>\n",
       "      <td>ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...</td>\n",
       "      <td>Society</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>267692</td>\n",
       "      <td>লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...</td>\n",
       "      <td>Organization</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184031</td>\n",
       "      <td>ওহা ভবনের দালাল</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>939131</td>\n",
       "      <td>আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210284</td>\n",
       "      <td>কি সাংঘাতিক ভাই রে তুই</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>350971</td>\n",
       "      <td>পুরোনো ইতিহাস তুলে ধরার জন্য সময় সংবাদ কে ধন্...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>539053</td>\n",
       "      <td>এই জন্যই আমাদের মেয়েরা কোরিয়া চলে যেতে চায়</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>200314</td>\n",
       "      <td>এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়</td>\n",
       "      <td>Individual</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>788171</td>\n",
       "      <td>আমি কিনে ফেলছি আই ফোন ১৪</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>562864</td>\n",
       "      <td>ব্যাংক কর্মকর্তা প্রকাশ্যে বলছেন ব্যাংক লোনের ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text         label  \\\n",
       "0     166449  ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...       Society   \n",
       "1     267692  লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...  Organization   \n",
       "2     184031                                    ওহা ভবনের দালাল          None   \n",
       "3     939131  আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...    Individual   \n",
       "4     210284                             কি সাংঘাতিক ভাই রে তুই    Individual   \n",
       "...      ...                                                ...           ...   \n",
       "2507  350971  পুরোনো ইতিহাস তুলে ধরার জন্য সময় সংবাদ কে ধন্...          None   \n",
       "2508  539053         এই জন্যই আমাদের মেয়েরা কোরিয়া চলে যেতে চায়          None   \n",
       "2509  200314         এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়    Individual   \n",
       "2510  788171                           আমি কিনে ফেলছি আই ফোন ১৪          None   \n",
       "2511  562864  ব্যাংক কর্মকর্তা প্রকাশ্যে বলছেন ব্যাংক লোনের ...          None   \n",
       "\n",
       "      toxic  label_id  \n",
       "0         1         1  \n",
       "1         1         2  \n",
       "2         0         0  \n",
       "3         1         4  \n",
       "4         1         4  \n",
       "...     ...       ...  \n",
       "2507      0         0  \n",
       "2508      0         0  \n",
       "2509      1         4  \n",
       "2510      0         0  \n",
       "2511      0         0  \n",
       "\n",
       "[2512 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = train_df\n",
    "\n",
    "df_val = dev_df\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affe6a57-3c6a-4360-aacd-1d27e2c9b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 36778\n",
      "Validation size: 1256\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # existing train and dev\n",
    "# df_train = train_df.copy()\n",
    "# df_val   = dev_df.copy()\n",
    "\n",
    "# # split dev_df in half\n",
    "# half = len(df_val) // 2\n",
    "# dev_train = df_val.iloc[:half].reset_index(drop=True)\n",
    "# dev_val   = df_val.iloc[half:].reset_index(drop=True)\n",
    "\n",
    "# # add first half of dev to training set\n",
    "# df_train = pd.concat([df_train, dev_train], ignore_index=True)\n",
    "\n",
    "# # final validation set is the second half\n",
    "# df_val = dev_val\n",
    "\n",
    "# print(\"Train size:\", len(df_train))\n",
    "# print(\"Validation size:\", len(df_val))\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# existing train and dev\n",
    "df_train = train_df.copy()\n",
    "df_val   = dev_df.copy()\n",
    "\n",
    "# --- 75/25 split of dev_df ---\n",
    "n   = len(df_val)\n",
    "cut = int(0.5 * n)  # floor to an integer\n",
    "\n",
    "# (optional) shuffle to avoid order bias:\n",
    "# df_val = df_val.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "dev_train = df_val.iloc[:cut].reset_index(drop=True)  # 75%\n",
    "dev_val   = df_val.iloc[cut:].reset_index(drop=True)  # 25%\n",
    "\n",
    "# add 75% of dev to training set\n",
    "df_train = pd.concat([df_train, dev_train], ignore_index=True)\n",
    "\n",
    "# final validation set is the remaining 25%\n",
    "df_val = dev_val\n",
    "\n",
    "print(\"Train size:\", len(df_train))\n",
    "print(\"Validation size:\", len(df_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25442c3b-3a5b-471f-afed-0daac93380cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Categories: ['Community', 'Individual', 'None', 'Organization', 'Society']\n"
     ]
    }
   ],
   "source": [
    "toxic_df = df_train\n",
    "target_list = sorted(toxic_df['label'].unique().tolist()) # Sort for consistent column order\n",
    "print(f\"Target Categories: {target_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f447313-7260-4da0-ba2a-aa24bc0697c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Community</th>\n",
       "      <th>Individual</th>\n",
       "      <th>None</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Society</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>মিথ্যা নাটক করে লাভ নেই সাজানো নাটক শুরু হয়ে গেছে</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ঘন্টা বাজাতে দফতরের কাজ টা কি আপনি নেবেন নাকি</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ইতিহাসের দীর্ঘতম বিসিএস শেষে মেধাবীদের সাথে এভ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>সময় টিভি আসলে কি চায় ভারত সিদ্ধান্ত মেনে নিলে ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>আমেরিকার হচ্ছে শয়তানের দাদর বাড়ি</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>পুরোনো ইতিহাস তুলে ধরার জন্য সময় সংবাদ কে ধন্...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>এই জন্যই আমাদের মেয়েরা কোরিয়া চলে যেতে চায়</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>আমি কিনে ফেলছি আই ফোন ১৪</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>ব্যাংক কর্মকর্তা প্রকাশ্যে বলছেন ব্যাংক লোনের ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1256 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  Community  \\\n",
       "0     মিথ্যা নাটক করে লাভ নেই সাজানো নাটক শুরু হয়ে গেছে      False   \n",
       "1         ঘন্টা বাজাতে দফতরের কাজ টা কি আপনি নেবেন নাকি      False   \n",
       "2     ইতিহাসের দীর্ঘতম বিসিএস শেষে মেধাবীদের সাথে এভ...      False   \n",
       "3     সময় টিভি আসলে কি চায় ভারত সিদ্ধান্ত মেনে নিলে ...      False   \n",
       "4                      আমেরিকার হচ্ছে শয়তানের দাদর বাড়ি      False   \n",
       "...                                                 ...        ...   \n",
       "1251  পুরোনো ইতিহাস তুলে ধরার জন্য সময় সংবাদ কে ধন্...      False   \n",
       "1252         এই জন্যই আমাদের মেয়েরা কোরিয়া চলে যেতে চায়      False   \n",
       "1253         এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়      False   \n",
       "1254                           আমি কিনে ফেলছি আই ফোন ১৪      False   \n",
       "1255  ব্যাংক কর্মকর্তা প্রকাশ্যে বলছেন ব্যাংক লোনের ...      False   \n",
       "\n",
       "      Individual   None  Organization  Society  \n",
       "0           True  False         False    False  \n",
       "1          False   True         False    False  \n",
       "2          False  False          True    False  \n",
       "3          False  False          True    False  \n",
       "4          False  False          True    False  \n",
       "...          ...    ...           ...      ...  \n",
       "1251       False   True         False    False  \n",
       "1252       False   True         False    False  \n",
       "1253        True  False         False    False  \n",
       "1254       False   True         False    False  \n",
       "1255       False   True         False    False  \n",
       "\n",
       "[1256 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.get_dummies(df_train, columns=['label'], prefix='', prefix_sep='')[['text'] + target_list]\n",
    "\n",
    "df_val = pd.get_dummies(df_val, columns=['label'], prefix='', prefix_sep='')[['text'] + target_list]\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fe7065-bf0f-46f6-9800-56c44df61496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as tq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from torch.optim import AdamW  # <-- use this instead\n",
    "\n",
    "\n",
    "MODEL_NAME =\"csebuetnlp/banglabert_large\"\n",
    "# Use a pipeline as a high-level helper\n",
    "#MODEL_NAME = \"google-bert/bert-base-multilingual-cased\"\n",
    "\n",
    "#MODEL_NAME=\"FacebookAI/xlm-roberta-large\"\n",
    "\n",
    "# You can keep these as they are or tune them\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 16\n",
    "EPOCHS = 7\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f5a343a-fa60-406b-98fd-003586889405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as tq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        # Corrected column name from 'title' or 'Text' to 'text'\n",
    "        self.texts = list(df['text']) \n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN, target_list)\n",
    "val_dataset = CustomDataset(df_val, tokenizer, MAX_LEN, target_list)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b025f33-4bfe-407e-b823-1a2c6062018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "# # Your existing configuration\n",
    "# MODEL_NAME = \"csebuetnlp/banglabert\"\n",
    "# MAX_LEN = 256\n",
    "# TRAIN_BATCH_SIZE = 16\n",
    "# VAL_BATCH_SIZE = 16\n",
    "# EPOCHS = 7\n",
    "# LEARNING_RATE = 2e-5\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# TACT specific hyperparameters\n",
    "EPSILON = 0.01  # Perturbation factor\n",
    "LAMBDA = 0.3    # Balance parameter between BCE and InfoNCE\n",
    "TEMPERATURE = 0.07  # Temperature for InfoNCE loss\n",
    "\n",
    "class TACTBanglaBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim=768):\n",
    "        super(TACTBanglaBERT, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Multi-label classification head\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Non-linear projection layer for contrastive learning\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 128)  # Final projection dimension\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, targets=None, apply_tact=True):\n",
    "        # Get BERT embeddings\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation from last hidden state\n",
    "        # BanglaBERT doesn't have pooler_output, so we use the [CLS] token (first token)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # [CLS] token is at index 0\n",
    "        \n",
    "        if apply_tact and targets is not None and self.training:\n",
    "            return self.forward_with_tact(pooled_output, targets)\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.classifier(pooled_output)\n",
    "            \n",
    "            loss = None\n",
    "            if targets is not None:\n",
    "                # Multi-label binary cross entropy\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "                \n",
    "            return {\n",
    "                \"loss\": loss, \n",
    "                \"logits\": logits, \n",
    "                \"hidden_states\": pooled_output\n",
    "            }\n",
    "    \n",
    "    def forward_with_tact(self, original_repr, targets):\n",
    "        \"\"\"\n",
    "        Implements TACT training with token-level adversarial perturbations\n",
    "        \"\"\"\n",
    "        # Enable gradients for token representations\n",
    "        original_repr = original_repr.requires_grad_(True)\n",
    "        \n",
    "        # Initial forward pass to compute gradients\n",
    "        pooled_output = self.dropout(original_repr)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        # Compute binary cross-entropy loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        \n",
    "        # Compute gradients w.r.t. token representations\n",
    "        grad_outputs = torch.ones_like(bce_loss)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=bce_loss,\n",
    "            inputs=original_repr,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        # Calculate adversarial perturbation (Equation 5 from TACT paper)\n",
    "        grad_norm = torch.norm(grad, dim=-1, keepdim=True)\n",
    "        grad_norm = torch.clamp(grad_norm, min=1e-8)  # Avoid division by zero\n",
    "        perturbation = -EPSILON * (grad / grad_norm)\n",
    "        \n",
    "        # Generate perturbed representation (Equation 6)\n",
    "        perturbed_repr = original_repr + perturbation\n",
    "        \n",
    "        # Forward pass with original representation\n",
    "        pooled_original = self.dropout(original_repr)\n",
    "        logits_original = self.classifier(pooled_original)\n",
    "        proj_original = self.projection(pooled_original)\n",
    "        \n",
    "        # Forward pass with perturbed representation  \n",
    "        pooled_perturbed = self.dropout(perturbed_repr)\n",
    "        logits_perturbed = self.classifier(pooled_perturbed)\n",
    "        proj_perturbed = self.projection(pooled_perturbed)\n",
    "        \n",
    "        # Compute losses\n",
    "        bce_loss_original = F.binary_cross_entropy_with_logits(logits_original, targets)\n",
    "        bce_loss_perturbed = F.binary_cross_entropy_with_logits(logits_perturbed, targets)\n",
    "        \n",
    "        # InfoNCE Loss for contrastive learning\n",
    "        infonce_loss = self.compute_infonce_loss(proj_original, proj_perturbed)\n",
    "        \n",
    "        # Combined loss (Equation 7 from TACT paper)\n",
    "        total_loss = (1 - LAMBDA) / 2 * (bce_loss_original + bce_loss_perturbed) + LAMBDA * infonce_loss\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"logits\": logits_original,\n",
    "            \"hidden_states\": pooled_original,\n",
    "            \"bce_loss_original\": bce_loss_original,\n",
    "            \"bce_loss_perturbed\": bce_loss_perturbed,\n",
    "            \"infonce_loss\": infonce_loss\n",
    "        }\n",
    "    \n",
    "    def compute_infonce_loss(self, z1, z2):\n",
    "        \"\"\"\n",
    "        Compute InfoNCE loss for contrastive learning\n",
    "        \"\"\"\n",
    "        batch_size = z1.size(0)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        z1_norm = F.normalize(z1, dim=-1)\n",
    "        z2_norm = F.normalize(z2, dim=-1)\n",
    "        \n",
    "        # Concatenate representations\n",
    "        representations = torch.cat([z1_norm, z2_norm], dim=0)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(representations, representations.T) / TEMPERATURE\n",
    "        \n",
    "        # Create positive pair labels\n",
    "        labels = torch.cat([torch.arange(batch_size, 2*batch_size), \n",
    "                           torch.arange(0, batch_size)]).to(device)\n",
    "        \n",
    "        # Mask diagonal (self-similarity)\n",
    "        mask = torch.eye(2*batch_size, dtype=torch.bool, device=device)\n",
    "        similarity_matrix.masked_fill_(mask, -float('inf'))\n",
    "        \n",
    "        # InfoNCE loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class TACTTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, scheduler=None):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.best_accuracy = 0.0\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_bce_original = 0\n",
    "        total_bce_perturbed = 0\n",
    "        total_infonce = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tq(self.train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with TACT\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                targets=targets,\n",
    "                apply_tact=True\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            if 'bce_loss_original' in outputs:\n",
    "                total_bce_original += outputs['bce_loss_original'].item()\n",
    "            if 'bce_loss_perturbed' in outputs:\n",
    "                total_bce_perturbed += outputs['bce_loss_perturbed'].item()\n",
    "            if 'infonce_loss' in outputs:\n",
    "                total_infonce += outputs['infonce_loss'].item()\n",
    "            \n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        metrics = {\n",
    "            'avg_loss': total_loss / num_batches,\n",
    "            'avg_bce_original': total_bce_original / num_batches,\n",
    "            'avg_bce_perturbed': total_bce_perturbed / num_batches,\n",
    "            'avg_infonce': total_infonce / num_batches\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tq(self.val_loader, desc=\"Validation\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                # Forward pass without TACT\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    targets=targets,\n",
    "                    apply_tact=False\n",
    "                )\n",
    "                \n",
    "                loss = outputs['loss']\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = outputs['logits']\n",
    "                predictions = torch.sigmoid(logits)\n",
    "                predicted_labels = (predictions > 0.5).float()\n",
    "                \n",
    "                all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                progress_bar.set_postfix({'Val Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_targets = np.array(all_targets)\n",
    "        \n",
    "        # Exact match accuracy (all labels must match)\n",
    "        exact_match_accuracy = accuracy_score(all_targets, all_predictions)\n",
    "        \n",
    "        # Per-label accuracy\n",
    "        label_accuracies = []\n",
    "        for i in range(all_targets.shape[1]):\n",
    "            label_acc = accuracy_score(all_targets[:, i], all_predictions[:, i])\n",
    "            label_accuracies.append(label_acc)\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        return avg_loss, exact_match_accuracy, label_accuracies\n",
    "\n",
    "def train_tact_model(train_data_loader, val_data_loader, target_list, num_epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Train TACT model using your existing data loaders\n",
    "    \"\"\"\n",
    "    print(f\"Training TACT model with {len(target_list)} labels: {target_list}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TACTBanglaBERT(MODEL_NAME, len(target_list)).to(device)\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), \n",
    "        lr=LEARNING_RATE, \n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(train_data_loader) * num_epochs\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=1.0, \n",
    "        end_factor=0.1, \n",
    "        total_iters=total_steps\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = TACTTrainer(model, train_data_loader, val_data_loader, optimizer, scheduler)\n",
    "    \n",
    "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Training loop\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = trainer.train_epoch()\n",
    "        print(f\"Training - Loss: {train_metrics['avg_loss']:.4f}, \"\n",
    "              f\"BCE Orig: {train_metrics['avg_bce_original']:.4f}, \"\n",
    "              f\"BCE Pert: {train_metrics['avg_bce_perturbed']:.4f}, \"\n",
    "              f\"InfoNCE: {train_metrics['avg_infonce']:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, exact_accuracy, label_accuracies = trainer.evaluate()\n",
    "        print(f\"Validation - Loss: {val_loss:.4f}, Exact Match Accuracy: {exact_accuracy:.4f}\")\n",
    "        \n",
    "        # Print per-label accuracies\n",
    "        print(\"Per-label accuracies:\")\n",
    "        for i, (label_name, acc) in enumerate(zip(target_list, label_accuracies)):\n",
    "            print(f\"  {label_name}: {acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if exact_accuracy > best_accuracy:\n",
    "            best_accuracy = exact_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'target_list': target_list\n",
    "            }, 'best_tact_bangla_model.pth')\n",
    "            print(f\"✓ New best model saved! Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining completed! Best exact match accuracy: {best_accuracy:.4f}\")\n",
    "    return model, trainer\n",
    "\n",
    "# Inference function for your trained model\n",
    "def predict_with_tact(model, text, tokenizer, target_list, max_len=MAX_LEN, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained TACT model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            apply_tact=False\n",
    "        )\n",
    "        \n",
    "        # Get probabilities\n",
    "        logits = outputs['logits']\n",
    "        probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "        \n",
    "        # Create result dictionary\n",
    "        results = {}\n",
    "        for i, (label, prob) in enumerate(zip(target_list, probabilities)):\n",
    "            results[label] = {\n",
    "                'probability': float(prob),\n",
    "                'predicted': bool(prob > threshold)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage with your existing setup:\n",
    "\"\"\"\n",
    "# Assuming you already have:\n",
    "# - df_train, df_val dataframes\n",
    "# - target_list (e.g., ['Community', 'Individual', 'None', 'Organization', 'Society'])\n",
    "# - train_data_loader, val_data_loader from your CustomDataset\n",
    "\n",
    "# Train the TACT model\n",
    "model, trainer = train_tact_model(train_data_loader, val_data_loader, target_list)\n",
    "\n",
    "# Make predictions\n",
    "sample_text = \"আমেরিকার হচ্ছে শয়তানের দাদর বাড়ি\"\n",
    "predictions = predict_with_tact(model, sample_text, tokenizer, target_list)\n",
    "print(predictions)\n",
    "\n",
    "# Load saved model for inference\n",
    "checkpoint = torch.load('best_tact_bangla_model.pth')\n",
    "model = TACTBanglaBERT(MODEL_NAME, len(target_list)).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\"\"\"\n",
    "\n",
    "# Usage with your existing setup:\n",
    "\n",
    "# Assuming you already have:\n",
    "# - df_train, df_val dataframes\n",
    "# - target_list (e.g., ['Community', 'Individual', 'None', 'Organization', 'Society'])\n",
    "# - train_data_loader, val_data_loader from your CustomDataset\n",
    "\n",
    "# Train the TACT model\n",
    "model, trainer = train_tact_model(train_data_loader, val_data_loader, target_list)\n",
    "\n",
    "# Make predictions\n",
    "# sample_text = \"আমেরিকার হচ্ছে শয়তানের দাদর বাড়ি\"\n",
    "# predictions = predict_with_tact(model, sample_text, tokenizer, target_list)\n",
    "# print(predictions)\n",
    "\n",
    "# # Load saved model for inference\n",
    "# checkpoint = torch.load('best_tact_bangla_model.pth')\n",
    "# model = TACTBanglaBERT(MODEL_NAME, len(target_list)).to(device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a8f3d89-e56b-4427-9856-2cb44e3fda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# load checkpoint + target list\n",
    "checkpoint = torch.load('best_tact_bangla_model.pth', map_location=device)\n",
    "target_list = checkpoint['target_list']\n",
    "num_labels = len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb982879-70e5-4af6-9585-1a0aad60e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "# dataloader\n",
    "test_ds = TestDataset(test_df['text'].tolist(), tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_ds, batch_size=VAL_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b5a04f1-6dae-4df2-a557-db0caf978743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Pipeline on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TACTBanglaBERT(\n",
       "  (bert): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=192, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Evaluating Pipeline on Test Set ---\")\n",
    "\n",
    "model = TACTBanglaBERT(MODEL_NAME, num_labels).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d92966c0-6be9-48d8-a78e-a8e4af3b5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs_all = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch.get('token_type_ids')\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids, device=device)\n",
    "        else:\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            apply_tact=False\n",
    "        )\n",
    "        logits = out['logits']\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()  # shape [B, num_labels]\n",
    "        probs_all.append(probs)\n",
    "\n",
    "probs_all = np.vstack(probs_all)  # shape [N, num_labels]\n",
    "\n",
    "# MAX-PROB (Top-1) labeling\n",
    "top1_idx   = probs_all.argmax(axis=1)\n",
    "top1_label = [target_list[i] for i in top1_idx]\n",
    "top1_prob  = probs_all.max(axis=1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Assemble output and save\n",
    "# ---------------------------------------------------------------------\n",
    "prob_cols = [f'prob_{l}' for l in target_list]\n",
    "pred_df = pd.DataFrame(probs_all, columns=prob_cols)\n",
    "pred_df['pred_label'] = top1_label\n",
    "pred_df['pred_prob']  = top1_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5b748a0-1f1a-4235-8f70-231993486889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_Community</th>\n",
       "      <th>prob_Individual</th>\n",
       "      <th>prob_None</th>\n",
       "      <th>prob_Organization</th>\n",
       "      <th>prob_Society</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077769</td>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.097183</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.818726</td>\n",
       "      <td>Society</td>\n",
       "      <td>0.818726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.945504</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>None</td>\n",
       "      <td>0.945504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266807</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.669219</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>0.021974</td>\n",
       "      <td>None</td>\n",
       "      <td>0.669219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771336</td>\n",
       "      <td>0.049234</td>\n",
       "      <td>0.198474</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>Community</td>\n",
       "      <td>0.771336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.312005</td>\n",
       "      <td>0.614860</td>\n",
       "      <td>0.091083</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.030483</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.614860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>0.228129</td>\n",
       "      <td>0.161033</td>\n",
       "      <td>0.031235</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.606961</td>\n",
       "      <td>Society</td>\n",
       "      <td>0.606961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>0.012582</td>\n",
       "      <td>0.967270</td>\n",
       "      <td>0.027649</td>\n",
       "      <td>0.011332</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.967270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>0.155134</td>\n",
       "      <td>0.856026</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.010399</td>\n",
       "      <td>0.056156</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.856026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.907622</td>\n",
       "      <td>0.051957</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>None</td>\n",
       "      <td>0.907622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.987594</td>\n",
       "      <td>0.004825</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>None</td>\n",
       "      <td>0.987594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prob_Community  prob_Individual  prob_None  prob_Organization  \\\n",
       "0            0.077769         0.015923   0.097183           0.035300   \n",
       "1            0.005938         0.063930   0.945504           0.002170   \n",
       "2            0.266807         0.010014   0.669219           0.027643   \n",
       "3            0.771336         0.049234   0.198474           0.017351   \n",
       "4            0.312005         0.614860   0.091083           0.004351   \n",
       "...               ...              ...        ...                ...   \n",
       "10195        0.228129         0.161033   0.031235           0.022525   \n",
       "10196        0.012582         0.967270   0.027649           0.011332   \n",
       "10197        0.155134         0.856026   0.035680           0.010399   \n",
       "10198        0.022920         0.012747   0.907622           0.051957   \n",
       "10199        0.006654         0.003993   0.987594           0.004825   \n",
       "\n",
       "       prob_Society  pred_label  pred_prob  \n",
       "0          0.818726     Society   0.818726  \n",
       "1          0.002271        None   0.945504  \n",
       "2          0.021974        None   0.669219  \n",
       "3          0.029835   Community   0.771336  \n",
       "4          0.030483  Individual   0.614860  \n",
       "...             ...         ...        ...  \n",
       "10195      0.606961     Society   0.606961  \n",
       "10196      0.011724  Individual   0.967270  \n",
       "10197      0.056156  Individual   0.856026  \n",
       "10198      0.034911        None   0.907622  \n",
       "10199      0.011018        None   0.987594  \n",
       "\n",
       "[10200 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "116ec401-04c4-47aa-8bb1-daf6f3f8ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# #'Abusive', 'None', 'Political Hate', 'Profane', 'Religious Hate', 'Sexism'\n",
    "# # #'Abusive', 'Political Hate', 'Profane', 'Religious Hate', 'Sexism']\n",
    "# # 'Community', 'Individual', 'None', 'Organization', 'Society'\n",
    "# # ['Community', 'Individual', 'None', 'Organization', 'Society']\n",
    "# # Your mapping\n",
    "# id2l = {\n",
    "#     0: 'Community',\n",
    "#     1: 'Individual',\n",
    "#     2: 'None',\n",
    "#     3: 'Organization',\n",
    "#     4: 'Society'\n",
    "   \n",
    "# }\n",
    "\n",
    "# # Example y_pred\n",
    "# # y_pred = np.array([[0,0,0,0,0],[0,0,0,1,0],[1,0,0,0,0]])\n",
    "\n",
    "# def decode_labels(row):\n",
    "#     indices = np.where(row == 1)[0]\n",
    "#     if len(indices) == 0:\n",
    "#         return \"None\"\n",
    "#     # If multiple labels, join them with comma\n",
    "#     return \", \".join([id2l[i] for i in indices])\n",
    "\n",
    "# # Convert predictions into a DataFrame column\n",
    "# df = pd.DataFrame()\n",
    "# df[\"Predicted_Label\"] = [decode_labels(row) for row in y_pred]\n",
    "\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7237c560-cb2b-4726-9a9f-d069de6e7835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Society', 'None', 'Community', 'Individual', 'Organization'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label']=pred_df['pred_label']\n",
    "test_df['model']='TACTBanglaBERT'\n",
    "test_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71935ab6-9e44-4910-bf21-b579e616c319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12764</td>\n",
       "      <td>Society</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202933</td>\n",
       "      <td>None</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165894</td>\n",
       "      <td>None</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124999</td>\n",
       "      <td>Community</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>535301</td>\n",
       "      <td>Individual</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>908819</td>\n",
       "      <td>Society</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>597085</td>\n",
       "      <td>Individual</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>901448</td>\n",
       "      <td>Individual</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>617821</td>\n",
       "      <td>None</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>316207</td>\n",
       "      <td>None</td>\n",
       "      <td>TACTBanglaBERT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       label           model\n",
       "0       12764     Society  TACTBanglaBERT\n",
       "1      202933        None  TACTBanglaBERT\n",
       "2      165894        None  TACTBanglaBERT\n",
       "3      124999   Community  TACTBanglaBERT\n",
       "4      535301  Individual  TACTBanglaBERT\n",
       "...       ...         ...             ...\n",
       "10195  908819     Society  TACTBanglaBERT\n",
       "10196  597085  Individual  TACTBanglaBERT\n",
       "10197  901448  Individual  TACTBanglaBERT\n",
       "10198  617821        None  TACTBanglaBERT\n",
       "10199  316207        None  TACTBanglaBERT\n",
       "\n",
       "[10200 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df[['id', 'label', 'model']]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "756f058f-d25f-4a80-87b2-6ba014e4f014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to final_ensemble.tsv\n"
     ]
    }
   ],
   "source": [
    "test_df.to_csv(\"V6_subtask_2A.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Saved to final_ensemble.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09fbae-5e02-4849-b6c6-4bf1261003e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
