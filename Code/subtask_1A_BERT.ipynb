{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Noik9q9c7Bhm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [Hate Speech Identification Shared Task](https://multihate.github.io/): Subtask 1A at [BLP Workshop](https://blp-workshop.github.io/) @IJCNLP-AACL 2025\n",
    "\n",
    "This shared task is designed to identify the type of hate, its severity, and the targeted group from social media content. The goal is to develop robust systems that advance research in this area.\n",
    "\n",
    "In this subtask, given a Bangla text collected from YouTube comments, categorize whether it contains abusive, sexism, religious hate, political hate, profane, or none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSxBhCps7oBf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Downloading dataset from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvwQNYHk6kV5",
    "outputId": "b67b70b4-66ed-45b6-d3ce-f92f1561ec9a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-18 12:36:11--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8002036 (7.6M) [text/plain]\n",
      "Saving to: â€˜blp25_hatespeech_subtask_1A_train.tsv.2â€™\n",
      "\n",
      "blp25_hatespeech_su 100%[===================>]   7.63M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-08-18 12:36:12 (94.8 MB/s) - â€˜blp25_hatespeech_subtask_1A_train.tsv.2â€™ saved [8002036/8002036]\n",
      "\n",
      "--2025-08-18 12:36:12--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 585339 (572K) [text/plain]\n",
      "Saving to: â€˜blp25_hatespeech_subtask_1A_dev.tsv.2â€™\n",
      "\n",
      "blp25_hatespeech_su 100%[===================>] 571.62K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-08-18 12:36:13 (18.4 MB/s) - â€˜blp25_hatespeech_subtask_1A_dev.tsv.2â€™ saved [585339/585339]\n",
      "\n",
      "--2025-08-18 12:36:13--  https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev_test.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 548258 (535K) [text/plain]\n",
      "Saving to: â€˜blp25_hatespeech_subtask_1A_dev_test.tsv.1â€™\n",
      "\n",
      "blp25_hatespeech_su 100%[===================>] 535.41K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-08-18 12:36:13 (18.8 MB/s) - â€˜blp25_hatespeech_subtask_1A_dev_test.tsv.1â€™ saved [548258/548258]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_train.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev_test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYZ96DWt-TZk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### installing required libraries.\n",
    " - transformers\n",
    " - datasets\n",
    " - evaluate\n",
    " - accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLJh5GGU-xET",
    "outputId": "0b7cdf80-809c-40fb-c01e-f13fb26e2ee0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /users/t/p/tprama/.local/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /users/t/p/tprama/.local/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /users/t/p/tprama/.local/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /users/t/p/tprama/.local/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from evaluate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /users/t/p/tprama/.local/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /users/t/p/tprama/.local/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from evaluate) (0.29.1)\n",
      "Requirement already satisfied: packaging in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /users/t/p/tprama/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "# !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhVWUJ3A_hx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### importing required libraries and setting up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VIUAU0rRBOmR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP6CdL7NHpxJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bMzfE34iHyGV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_file = 'blp25_hatespeech_subtask_1A_train.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_dev_test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w59H3fOnLUcG"
   },
   "source": [
    "### Disable wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CRQSQF6MLYrB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-_w4YehCgX4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-GUUNj0BPbu",
    "outputId": "3d937d6e-b079-4fbc-d34b-9a6fe5731636",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BERT_HateSpeech/\",\n",
    "    \n",
    "    # Your core changes\n",
    "    num_train_epochs=15,  # Increased from 1 to 3. You can experiment with 4 or 5.\n",
    "    \n",
    "    # Important adjustments for finding the best model\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,  # This will load the best checkpoint when training is done\n",
    "    \n",
    "    # Other common parameters\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=2,           # Saves disk space by only keeping the best 2 checkpoints\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "max_train_samples = None\n",
    "max_eval_samples=None\n",
    "max_predict_samples=None\n",
    "max_seq_length = 512\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Q4deAnUJ0iI",
    "outputId": "192f38ba-296a-4026-fe2c-65b88d7d3a8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/20/2025 23:50:07 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0 distributed training: True, 16-bits training: False\n",
      "08/20/2025 23:50:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./BERT_HateSpeech/runs/Aug20_23-50-05_node414.cluster,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=15,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./BERT_HateSpeech/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./BERT_HateSpeech/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgkvwlbFHVo5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-De1tz5qHYre",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'aplycaebous/tb-BERT-fpt'\n",
    "\n",
    "#model_name = \"csebuetnlp/banglabert\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPqrrDbcKN8n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### setting the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZvKpoxaQKTB6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgNrs7AhKdvl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDwaW8AnKcgD",
    "outputId": "7709d37d-9226-4257-961c-878a6805ddd9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2025 12:50:50 - INFO - __main__ - loading a local file for train\n",
      "08/18/2025 12:50:50 - INFO - __main__ - loading a local file for validation\n",
      "08/18/2025 12:50:50 - INFO - __main__ - loading a local file for test\n"
     ]
    }
   ],
   "source": [
    "l2id = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n",
    "train_df = pd.read_csv(train_file, sep='\\t')\n",
    "# print(train_df['label'])\n",
    "train_df['label'] = train_df['label'].map(l2id).fillna(0).astype(int)\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "validation_df = pd.read_csv(validation_file, sep='\\t')\n",
    "validation_df['label'] = validation_df['label'].map(l2id).fillna(0).astype(int)\n",
    "validation_df = Dataset.from_pandas(validation_df)\n",
    "test_df = pd.read_csv(test_file, sep='\\t')\n",
    "#test_df['label'] = test_df['label'].map(l2id)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "data_files = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
    "for key in data_files.keys():\n",
    "    logger.info(f\"loading a local file for {key}\")\n",
    "raw_datasets = DatasetDict(\n",
    "    {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1swECfaTuJl",
    "outputId": "44f72ccb-9e88-4bf8-e6d6-dbbafe33acc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJhNu7tPQ2RU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Extracting number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTl6NNPmOXhO",
    "outputId": "d8fde20b-5597-4289-e0db-cb17047f6ba5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 4, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "print(label_list)\n",
    "label_list.sort()  # sort the labels for determine\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1dpoOAPRJnN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Pretrained Configuration, Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmAaMuBuRQd2",
    "outputId": "e3e81d02-d08a-4fb1-fdbf-610592068373",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:699] 2025-08-18 12:50:52,757 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--sagorsarker--bangla-bert-base/snapshots/875aa80a42ec196c16bd931ae5d85ad949f58b16/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-18 12:50:52,758 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"sagorsarker/bangla-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102025\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:730] 2025-08-18 12:50:52,794 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:699] 2025-08-18 12:50:52,845 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--sagorsarker--bangla-bert-base/snapshots/875aa80a42ec196c16bd931ae5d85ad949f58b16/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-18 12:50:52,846 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"sagorsarker/bangla-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102025\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-18 12:50:52,925 >> loading file vocab.txt from cache at /users/t/p/tprama/.cache/huggingface/hub/models--sagorsarker--bangla-bert-base/snapshots/875aa80a42ec196c16bd931ae5d85ad949f58b16/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-18 12:50:52,925 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-18 12:50:52,926 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-18 12:50:52,926 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-18 12:50:52,926 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2050] 2025-08-18 12:50:52,927 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:699] 2025-08-18 12:50:52,928 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--sagorsarker--bangla-bert-base/snapshots/875aa80a42ec196c16bd931ae5d85ad949f58b16/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-18 12:50:52,929 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"sagorsarker/bangla-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102025\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:699] 2025-08-18 12:50:53,159 >> loading configuration file config.json from cache at /users/t/p/tprama/.cache/huggingface/hub/models--sagorsarker--bangla-bert-base/snapshots/875aa80a42ec196c16bd931ae5d85ad949f58b16/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-08-18 12:50:53,160 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"sagorsarker/bangla-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102025\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3982] 2025-08-18 12:50:53,254 >> loading weights file model.safetensors from cache at /users/t/p/tprama/.cache/huggingface/hub/models--sagorsarker--bangla-bert-base/snapshots/875aa80a42ec196c16bd931ae5d85ad949f58b16/model.safetensors\n",
      "[INFO|modeling_utils.py:4960] 2025-08-18 12:50:55,210 >> Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4972] 2025-08-18 12:50:55,211 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Define your model name and number of labels\n",
    "model_name = \"sagorsarker/bangla-bert-base\"\n",
    "num_labels = 6 # Based on your l2id dictionary: None, Religious, Sexism, etc.\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "# Load the model FOR SEQUENCE CLASSIFICATION\n",
    "# This is the key change!\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True, # Important when adding a new head\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7PIQVypeTf4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preprocessing the raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "9c9e4b5efd4444b198de361e1e60ee98",
      "e8ff088d8aea429c8904ddf5e30b7423",
      "760e1f384c204fdabcb50dc8f7503543",
      "372f7be9f4a24d44a9d023ce1ab9ee10",
      "95d8ef13b0cd42f4bc2d719b815ce2cf",
      "88858792a8da4d819cc60ddb8e36f3f1",
      "4f96df8a32ec4b0f852022e8aff1b362",
      "050a768a36a5493a8bf34ada920c7c85",
      "54587b6751104015be6bd6f889cde658",
      "3f5fd14f5c754741b85280bb9f0b3d09",
      "db47077ed94c4d668b52f0ae10cb99a7",
      "7dbefe9fda5f4ecdb5b760965692a16f",
      "7e164a3ac04041728597e163e8b6baad",
      "cee42ee7383b441f83d96bdbfdeb42a8",
      "279cfb5747d34857866cb9fbea240d09",
      "6aad03f40bf543788b09f26e325e4b07",
      "4751966ad9b7434eb7384cf0acd1c779",
      "89395a91391047c08048cb333189f756",
      "38c98ec8e5da41adbe65aaa48023dc5c",
      "b1a37623e9474a47a8a1cb29a28a8bd9",
      "9b8815d16be24301aecfc2dbf891802b",
      "84b7d5a8cdb84f079effe447622005a5",
      "71e61ba4ad9443b7bcac4499e19b72a9",
      "dabaf6bf211540e2bbdd88f8e8e1ca85",
      "3935bb8e42ec41888a75d47a33354a25",
      "e80e199777024aaab80bf9520915fa4e",
      "7d565f30045543dca48996f9cd228520",
      "2533a698782c46eabe40b270ee278476",
      "e6865221d828480095a6ddd3c645a2e9",
      "03e39ae2f0d54efb9b19a4fadccf2c07",
      "5cd4d8331e1a43dc8481c316aa81222a",
      "3f8459b76cf14992a9148665e8cf5f6f",
      "93d21af9124f459ca29af0291e5f18bb"
     ]
    },
    "id": "pqO3YWAZelhd",
    "outputId": "bf447b90-67d7-4635-d4fc-196606b3fc1d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe57b6ec26044bab97c9728697607a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/35522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3892130ff2f4c89a63aa79d6f6d3f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f9ec50c8bb48d5b13d3296c74e70e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "sentence1_key= non_label_column_names[1]\n",
    "\n",
    "# Padding strategy\n",
    "padding = \"max_length\"\n",
    "\n",
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
    "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",)\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "if 128 > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({128}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\")\n",
    "max_seq_length = min(128, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],))\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASxWKiqifb_g",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the training data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QHoDqrBGgD6F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"train\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a train dataset\")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "if max_train_samples is not None:\n",
    "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqME25nm-hwo",
    "outputId": "83490003-2e87-472d-d619-3dc1c88a83cb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 35522\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k72vUTSigOzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the development/evaluation data for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "MqrW8ospgUYZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a validation dataset\")\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "if max_eval_samples is not None:\n",
    "    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7sVqp3hgU4i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the test data for predicting the unseen test data using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "u0dBjIQggcYs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
    "    raise ValueError(\"requires a test dataset\")\n",
    "predict_dataset = raw_datasets[\"test\"]\n",
    "if max_predict_samples is not None:\n",
    "    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n",
    "    predict_dataset = predict_dataset.select(range(max_predict_samples_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqbo1xzRge36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Log a few random samples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIO2bxSVgkLb",
    "outputId": "1baa6bce-15a4-4cf2-8616-d179a4395e8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2025 12:51:17 - INFO - __main__ - Sample 7296 of the training set: {'id': 660, 'text': 'à¦¸à¦°à¦•à¦¾à¦°à§‡à¦° à¦¦à¦¾à§Ÿà¦¿à¦¤à§à¦¬à¦ªà§à¦°à¦¾à¦ªà§à¦¤ à¦¸à¦‚à¦¸à§à¦¥à¦¾ à¦¸à¦•à¦² à¦¬à¦¾à¦£à¦¿à¦œà§à¦¯à¦¿à¦• à¦­à¦¬à¦¨ à¦—à§à¦²à§‹à¦•à§‡ à¦¬à¦›à¦°à§‡ à¦…à¦¨à§à¦¤à¦¤ à¦à¦•à¦¬à¦¾à¦° à¦ªà¦°à¦¿à¦¦à¦°à§à¦¶à¦¨ à¦•à¦°à§‡ à¦°à¦¿à¦ªà§‹à¦°à§à¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à¦¬à§‡à¦¨ à¦à¦•à¦Ÿà¦¾ à¦¬à¦¾à¦£à¦¿à¦œà§à¦¯à¦¿à¦• à¦­à¦¬à¦¨à§‡ à¦¨à¦¿à¦°à¦¾à¦ªà¦¤à§à¦¤à¦¾à¦° à¦¸à¦•à¦² à¦¬à§à¦¯à¦¬à¦¸à§à¦¥à¦¾ à¦¥à¦¾à¦•à¦¾ à¦‰à¦šà¦¿à¦¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦à¦‡ à¦¸à¦¬ à¦¸à¦®à§à¦­à¦¬à§‡à¦° à¦¦à§‡à¦¶à§‡ à¦•à¦¿à¦›à§à¦‡ à¦•à¦°à¦¾ à¦¹à§Ÿ à¦¨à¦¾ à¦à¦‡ à¦¦à§‡à¦¶à§‡ à¦®à¦¾à¦¨à§à¦·à§‡à¦° à¦œà§€à¦¬à¦¨à§‡à¦° à¦•à§‹à¦¨ à¦¦à¦¾à¦® à¦¨à§‡à¦‡ à¦¤à¦¾à¦‡ à¦•à§‡à¦‰ à¦•à§‹à¦¨à¦•à¦¿à¦›à§à¦° à¦¤à¦¦à¦¾à¦°à¦•à¦¿à¦°à¦“ à¦¦à¦°à¦•à¦¾à¦° à¦®à¦¨à§‡ à¦•à¦°à§‡ à¦¨à¦¾', 'label': 3, 'input_ids': [101, 2243, 6741, 33983, 59464, 28913, 34687, 67814, 12969, 3079, 100619, 33983, 3012, 3286, 13392, 17618, 2685, 5562, 63129, 3424, 10440, 23054, 5212, 2039, 39178, 2094, 3187, 4804, 2076, 10601, 2506, 2157, 100619, 33983, 3012, 5099, 21962, 19050, 63129, 3187, 3079, 5740, 69734, 2388, 12969, 2187, 2351, 77801, 2045, 2132, 7505, 58136, 2046, 2216, 2178, 7635, 2062, 2047, 33668, 2044, 2045, 2216, 3175, 47678, 2960, 2285, 5803, 2342, 2128, 2174, 2253, 2285, 5803, 6437, 7635, 2046, 18484, 2058, 2659, 2139, 2039, 2044, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "08/18/2025 12:51:17 - INFO - __main__ - Sample 1639 of the training set: {'id': 463531, 'text': 'à¦•à§‡ à¦•à§‡ à¦Ÿà¦¾ à¦°à§‹à¦œ à¦°à¦¾à¦–à¦¬à§‹ à¦à¦¬à¦‚ à¦ªà¦¾à¦à¦š à¦“à§Ÿà¦¾à¦•à§à¦¤ à¦¨à¦¾à¦®à¦¾à¦œ à¦†à¦¦à¦¾à§Ÿ à¦•à¦°à¦¬', 'label': 0, 'input_ids': [101, 2285, 2285, 2866, 4972, 18152, 6461, 2395, 2042, 34845, 2038, 61580, 3283, 5363, 6427, 9294, 3173, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "08/18/2025 12:51:17 - INFO - __main__ - Sample 18024 of the training set: {'id': 156766, 'text': 'à¦‰à¦¨à¦¾à¦¦à§‡à¦° à¦‰à¦šà§à¦›à§‡à¦¦ à¦•à¦°à¦¾ à¦‰à¦šà¦¿à¦¤ à¦¹à¦¬à§‡à¦¨à¦¾ à¦‰à¦¨à¦¾à¦°à¦¾ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦œà¦¾à¦¤à¦¿ à¦¸à¦¤à§à¦¤à¦¾à¦° à¦…à¦‚à¦¶', 'label': 0, 'input_ids': [101, 6825, 70071, 58245, 2047, 2351, 50954, 6796, 2127, 4503, 18368, 2202, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    }
   ],
   "source": [
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAcn0Pc8gogF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get the metric function `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "aMWMQdaUgvAq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWUyuBHgxbA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Predictions and label_ids field and has to return a dictionary string to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-3VqxkqcgxCC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNWK1Hfbg8-o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "_w6lNh-OhJLC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nYlugPRhNbg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Initialize our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "i-rwWO7wOpok"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(\"id\")\n",
    "eval_dataset = eval_dataset.remove_columns(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeJco0JOhPHx",
    "outputId": "97bf3501-d6f0-47b2-d075-c666ab52e1d8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1393713/1049306949.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUxWn9HrhqRM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "B681qnPFhtY0",
    "outputId": "6e337556-7e7d-48e3-cc5b-93d7453e14d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-18 12:51:26,921 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2405] 2025-08-18 12:51:26,929 >> ***** Running training *****\n",
      "[INFO|trainer.py:2406] 2025-08-18 12:51:26,929 >>   Num examples = 35,522\n",
      "[INFO|trainer.py:2407] 2025-08-18 12:51:26,930 >>   Num Epochs = 15\n",
      "[INFO|trainer.py:2408] 2025-08-18 12:51:26,930 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2410] 2025-08-18 12:51:26,930 >>   Training with DataParallel so batch size has been adjusted to: 32\n",
      "[INFO|trainer.py:2411] 2025-08-18 12:51:26,931 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2412] 2025-08-18 12:51:26,932 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2413] 2025-08-18 12:51:26,932 >>   Total optimization steps = 16,665\n",
      "[INFO|trainer.py:2414] 2025-08-18 12:51:26,933 >>   Number of trainable parameters = 164,401,158\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16665' max='16665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16665/16665 1:16:53, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.762226</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.751082</td>\n",
       "      <td>0.701035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>0.789739</td>\n",
       "      <td>0.700239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.900463</td>\n",
       "      <td>0.678742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>1.103938</td>\n",
       "      <td>0.675955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>1.415231</td>\n",
       "      <td>0.670382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>1.608553</td>\n",
       "      <td>0.662022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>1.940637</td>\n",
       "      <td>0.672373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>2.300166</td>\n",
       "      <td>0.664411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>2.540484</td>\n",
       "      <td>0.666003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>2.717161</td>\n",
       "      <td>0.667596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>2.839534</td>\n",
       "      <td>0.673169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>2.961327</td>\n",
       "      <td>0.674761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>3.025136</td>\n",
       "      <td>0.679140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>3.062358</td>\n",
       "      <td>0.676752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-18 12:55:06,280 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 12:55:06,285 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 12:55:06,285 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 12:55:06,286 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 12:55:12,633 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-1111\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 12:55:12,636 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-1111/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 12:55:13,769 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-1111/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 12:55:13,771 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-1111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 12:55:13,772 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-1111/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 12:55:16,000 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-4442] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 12:58:52,773 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 12:58:52,777 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 12:58:52,778 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 12:58:52,778 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 12:58:59,063 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-2222\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 12:58:59,066 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-2222/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 12:59:00,179 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-2222/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 12:59:00,181 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-2222/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 12:59:00,182 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-2222/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 12:59:02,376 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-1111] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:02:41,541 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:02:41,546 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:02:41,547 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:02:41,547 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:02:47,898 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-3333\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:02:47,902 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-3333/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:02:49,041 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-3333/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:02:49,066 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-3333/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:02:49,082 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-3333/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:02:51,261 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-3333] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:06:28,443 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:06:28,446 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:06:28,447 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:06:28,447 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:06:34,503 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-4444\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:06:34,506 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-4444/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:06:35,586 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-4444/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:06:35,588 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-4444/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:06:35,589 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-4444/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:06:37,856 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-4444] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:12:39,981 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:12:39,985 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:12:39,985 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:12:39,986 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:12:50,450 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-5555\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:12:50,454 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-5555/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:12:51,608 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-5555/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:12:51,611 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-5555/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:12:51,612 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-5555/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:12:53,885 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-5555] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:19:23,275 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:19:23,280 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:19:23,281 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:19:23,281 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:19:33,450 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-6666\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:19:33,454 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-6666/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:19:34,638 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-6666/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:19:34,654 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-6666/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:19:34,655 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-6666/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:19:36,876 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-6663] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:26:05,849 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:26:05,854 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:26:05,854 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:26:05,855 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:26:16,494 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-7777\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:26:16,496 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-7777/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:26:17,691 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-7777/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:26:17,693 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-7777/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:26:17,694 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-7777/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:26:19,943 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-6666] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:32:48,690 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:32:48,693 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:32:48,694 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:32:48,694 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:32:59,052 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-8888\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:32:59,055 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-8888/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:33:00,132 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-8888/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:33:00,148 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-8888/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:33:00,176 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-8888/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:33:02,419 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-7777] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:39:30,810 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:39:30,813 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:39:30,814 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:39:30,814 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:39:41,156 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-9999\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:39:41,159 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-9999/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:39:42,277 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-9999/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:39:42,279 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-9999/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:39:42,280 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-9999/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:39:44,566 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-8888] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:45:35,846 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:45:35,849 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:45:35,849 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:45:35,850 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:45:42,128 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-11110\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:45:42,131 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-11110/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:45:43,287 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-11110/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:45:43,289 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-11110/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:45:43,290 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-11110/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:45:45,552 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-9999] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:49:22,820 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:49:22,824 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:49:22,825 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:49:22,825 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:49:29,178 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-12221\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:49:29,181 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-12221/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:49:30,321 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-12221/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:49:30,323 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-12221/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:49:30,324 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-12221/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:49:32,478 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-11110] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:53:38,866 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:53:38,869 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:53:38,869 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:53:38,870 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:53:46,818 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-13332\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:53:46,821 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-13332/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:53:47,916 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-13332/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:53:47,918 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-13332/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:53:47,920 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-13332/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:53:50,240 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-12221] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 13:58:30,515 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 13:58:30,518 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 13:58:30,518 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 13:58:30,519 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 13:58:38,446 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-14443\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 13:58:38,448 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-14443/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 13:58:39,599 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-14443/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 13:58:39,601 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-14443/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 13:58:39,602 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-14443/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 13:58:41,828 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-13332] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:928] 2025-08-18 14:03:22,098 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 14:03:22,102 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 14:03:22,102 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 14:03:22,102 >>   Batch size = 32\n",
      "[INFO|trainer.py:3942] 2025-08-18 14:03:30,169 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-15554\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 14:03:30,171 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-15554/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 14:03:31,290 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-15554/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 14:03:31,292 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-15554/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 14:03:31,293 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-15554/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 14:03:33,489 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-14443] due to args.save_total_limit\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:3942] 2025-08-18 14:08:06,860 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-16665\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 14:08:06,863 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-16665/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 14:08:07,964 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-16665/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 14:08:07,966 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-16665/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 14:08:07,967 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-16665/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4034] 2025-08-18 14:08:10,196 >> Deleting older checkpoint [BanglaBERT_HateSpeech/checkpoint-15554] due to args.save_total_limit\n",
      "[INFO|trainer.py:928] 2025-08-18 14:08:10,199 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 14:08:10,202 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 14:08:10,202 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 14:08:10,203 >>   Batch size = 32\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:3942] 2025-08-18 14:08:18,300 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/checkpoint-16665\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 14:08:18,302 >> Configuration saved in ./BanglaBERT_HateSpeech/checkpoint-16665/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 14:08:19,434 >> Model weights saved in ./BanglaBERT_HateSpeech/checkpoint-16665/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 14:08:19,436 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/checkpoint-16665/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 14:08:19,437 >> Special tokens file saved in ./BanglaBERT_HateSpeech/checkpoint-16665/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2657] 2025-08-18 14:08:21,673 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2895] 2025-08-18 14:08:21,674 >> Loading best model from ./BanglaBERT_HateSpeech/checkpoint-2222 (score: 0.7510824799537659).\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = (\n",
    "    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaaRglkwllSp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Saving the tokenizer too for easy upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UwoMEbAloMx",
    "outputId": "4dd504fc-c8b3-4229-c111-7f16b1a81487",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3942] 2025-08-18 14:08:33,540 >> Saving model checkpoint to ./BanglaBERT_HateSpeech/\n",
      "[INFO|configuration_utils.py:423] 2025-08-18 14:08:33,542 >> Configuration saved in ./BanglaBERT_HateSpeech/config.json\n",
      "[INFO|modeling_utils.py:3040] 2025-08-18 14:08:34,742 >> Model weights saved in ./BanglaBERT_HateSpeech/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-08-18 14:08:34,744 >> tokenizer config file saved in ./BanglaBERT_HateSpeech/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2509] 2025-08-18 14:08:34,745 >> Special tokens file saved in ./BanglaBERT_HateSpeech/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n",
      "***** train metrics *****\n",
      "  epoch                    =       15.0\n",
      "  total_flos               = 32642506GF\n",
      "  train_loss               =     0.2274\n",
      "  train_runtime            = 1:16:56.06\n",
      "  train_samples            =      35522\n",
      "  train_samples_per_second =    115.429\n",
      "  train_steps_per_second   =       3.61\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9zCKBGEhwb7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Evaluating our model on validation/development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "YClw3dXTh17u",
    "outputId": "5936e42f-7648-47eb-e91c-68da6afda7a5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2025 14:08:41 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-18 14:08:41,836 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 14:08:41,839 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 14:08:41,840 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 14:08:41,840 >>   Batch size = 32\n",
      "/users/t/p/tprama/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       15.0\n",
      "  eval_accuracy           =      0.701\n",
      "  eval_loss               =     0.7511\n",
      "  eval_runtime            = 0:00:08.08\n",
      "  eval_samples            =       2512\n",
      "  eval_samples_per_second =    310.567\n",
      "  eval_steps_per_second   =      9.767\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "max_eval_samples = (\n",
    "    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",
    ")\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3LSdUdPh7uG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Predecting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "gnXhVq6Yh_oS",
    "outputId": "b8a5caab-bbc7-4772-bd74-2576761e3abb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2025 14:15:18 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:928] 2025-08-18 14:15:18,734 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4258] 2025-08-18 14:15:18,738 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4260] 2025-08-18 14:15:18,739 >>   Num examples = 2512\n",
      "[INFO|trainer.py:4263] 2025-08-18 14:15:18,739 >>   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/18/2025 14:15:25 - INFO - __main__ - ***** Predict results *****\n"
     ]
    }
   ],
   "source": [
    "id2l = {v: k for k, v in l2id.items()}\n",
    "logger.info(\"*** Predict ***\")\n",
    "#predict_dataset = predict_dataset.remove_columns(\"label\")\n",
    "ids = predict_dataset['id']\n",
    "predict_dataset = predict_dataset.remove_columns(\"id\")\n",
    "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "output_predict_file = os.path.join(training_args.output_dir, f\"subtask_1A.tsv\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_predict_file, \"w\") as writer:\n",
    "        logger.info(f\"***** Predict results *****\")\n",
    "        writer.write(\"id\\tlabel\\tmodel\\n\")\n",
    "        for index, item in enumerate(predictions):\n",
    "            item = label_list[item]\n",
    "            item = id2l[item]\n",
    "            writer.write(f\"{ids[index]}\\t{item}\\t{model_name}\\n\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Gqqk_24__47",
    "outputId": "38f16392-40ab-4277-ff5a-a333055bdcc2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879187"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQgoTTIoiI0X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Saving the model into card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1ooJgrViLVj",
    "outputId": "153992af-1ef8-4b26-9df4-25f4a0c328a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:449] 2025-08-18 14:15:29,611 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7010350227355957}]}\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\n",
    "trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2Uq2f0CQXvu",
    "outputId": "b1b2d9a3-9675-4e65-ae1f-e16c7c9bfbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tzip warning: name not matched: ./BanglaBERT_HateSpeech/task.tsv\n",
      "\n",
      "zip error: Nothing to do! (subtask_1A.zip)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!zip subtask_1A.zip ./BanglaBERT_HateSpeech/task.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNFwIyDfQ4Sl"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "zip subtask_1A.zip ./BanglaBERT_HateSpeech/subtask_1A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
