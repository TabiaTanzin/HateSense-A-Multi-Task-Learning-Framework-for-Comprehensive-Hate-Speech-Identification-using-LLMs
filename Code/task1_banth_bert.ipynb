{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a63a7f-1ae8-4f9d-8d7d-b3c7339a0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from torch.optim import AdamW  # <-- use this instead\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as tq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "MODEL_NAME =\"csebuetnlp/banglabert\"\n",
    "\n",
    "# You can keep these as they are or tune them\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0157876-41f3-4d68-969d-f492cb3cbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "182ddff0-5638-4dfa-8288-5dad145ac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'blp25_hatespeech_subtask_1A_train.tsv'\n",
    "validation_file = 'blp25_hatespeech_subtask_1A_dev.tsv'\n",
    "test_file = 'blp25_hatespeech_subtask_1A_test.tsv'\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# Load train/val/test DataFrames\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\")\n",
    "dev_df = pd.read_csv(validation_file , sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc20aa3-ad03-4b4f-8771-529634b73245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>toxic</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147963</td>\n",
       "      <td>ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214275</td>\n",
       "      <td>ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>849172</td>\n",
       "      <td>অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>821985</td>\n",
       "      <td>চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>477288</td>\n",
       "      <td>এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>790325</td>\n",
       "      <td>তইওয়ানের এত ক্ষমতা হয়নি যে এক টুকরো জায়গা নষ্ট...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>328377</td>\n",
       "      <td>চুরের ঘরের চুর হালা</td>\n",
       "      <td>Profane</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>69803</td>\n",
       "      <td>জাহাঙ্গীর বুদ্ধি নেই মাঠে মারা যাবে</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35520</th>\n",
       "      <td>419984</td>\n",
       "      <td>একটা ফেইল্ড এস্টেট এও সুষ্ঠু নির্বাচন হয় নেতার...</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35521</th>\n",
       "      <td>538747</td>\n",
       "      <td>ছেমরি হলো মাহফুজ রহমান কে ব্যাবহার করে কেরিয়ার...</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text    label  \\\n",
       "0      147963  ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...     None   \n",
       "1      214275  ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...     None   \n",
       "2      849172          অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে  Abusive   \n",
       "3      821985  চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...     None   \n",
       "4      477288  এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...  Abusive   \n",
       "...       ...                                                ...      ...   \n",
       "35517  790325  তইওয়ানের এত ক্ষমতা হয়নি যে এক টুকরো জায়গা নষ্ট...     None   \n",
       "35518  328377                                চুরের ঘরের চুর হালা  Profane   \n",
       "35519   69803                জাহাঙ্গীর বুদ্ধি নেই মাঠে মারা যাবে  Abusive   \n",
       "35520  419984  একটা ফেইল্ড এস্টেট এও সুষ্ঠু নির্বাচন হয় নেতার...  Abusive   \n",
       "35521  538747  ছেমরি হলো মাহফুজ রহমান কে ব্যাবহার করে কেরিয়ার...  Abusive   \n",
       "\n",
       "       toxic  label_id  \n",
       "0          0         0  \n",
       "1          0         0  \n",
       "2          1         5  \n",
       "3          0         0  \n",
       "4          1         5  \n",
       "...      ...       ...  \n",
       "35517      0         0  \n",
       "35518      1         4  \n",
       "35519      1         5  \n",
       "35520      1         5  \n",
       "35521      1         5  \n",
       "\n",
       "[35522 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2id = {\n",
    "    'None': 0,\n",
    "    'Religious Hate': 1,\n",
    "    'Sexism': 2,\n",
    "    'Political Hate': 3,\n",
    "    'Profane': 4,\n",
    "    'Abusive': 5\n",
    "}\n",
    "id2l = {v: k for k, v in l2id.items()}\n",
    "\n",
    "\n",
    "def clean_label(x):\n",
    "    # handle missing or NaN → \"None\"\n",
    "    if pd.isna(x) or x == 'None':\n",
    "        return 'None'\n",
    "    # already list-like e.g. ['Abusive']\n",
    "    if isinstance(x, list):\n",
    "        return x[0] if len(x) > 0 else 'None'\n",
    "    # string cases like \"[]\" or \"[Abusive]\" or \"[Political Hate]\"\n",
    "    x = x.strip(\"[]\").strip()\n",
    "    if x == \"\":\n",
    "        return 'None'\n",
    "    return x\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    # Ensure labels are proper lists\n",
    "    df[\"label\"] = df[\"label\"].apply(clean_label)\n",
    "    df[\"label\"] = df[\"label\"].fillna(\"None\")\n",
    "    # Now create binary label\n",
    "    df[\"toxic\"] = df[\"label\"].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "    df[\"label_id\"] = df[\"label\"].map(l2id)\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = process_df(train_df)\n",
    "dev_df  = process_df(dev_df)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee769340-e9b6-457e-9a8c-168769ddf29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>toxic</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166449</td>\n",
       "      <td>ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...</td>\n",
       "      <td>Political Hate</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>267692</td>\n",
       "      <td>লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>939131</td>\n",
       "      <td>আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210284</td>\n",
       "      <td>কি সাংঘাতিক ভাই রে তুই</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>712332</td>\n",
       "      <td>লঞ্চ মালিকদের অভিশপ্ত চক্ষু পদ্মা সেতুর উপর</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>653048</td>\n",
       "      <td>কিরে মানিক চোরা তুইও আছোস</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>121961</td>\n",
       "      <td>দেশের সবই তো চুরি হয়ে যাচ্ছে আর চোরদের ধরার কো...</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>555021</td>\n",
       "      <td>ফকিনি বাংলা দেশ কারেন্ট নাই</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>858412</td>\n",
       "      <td>কাকু সামনে চশমা থাকবে কিন্তু গ্লাস থাকবেনাপাছা...</td>\n",
       "      <td>Profane</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>200314</td>\n",
       "      <td>এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1061 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0     166449  ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...   \n",
       "1     267692  লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...   \n",
       "3     939131  আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...   \n",
       "4     210284                             কি সাংঘাতিক ভাই রে তুই   \n",
       "5     712332        লঞ্চ মালিকদের অভিশপ্ত চক্ষু পদ্মা সেতুর উপর   \n",
       "...      ...                                                ...   \n",
       "2496  653048                          কিরে মানিক চোরা তুইও আছোস   \n",
       "2503  121961  দেশের সবই তো চুরি হয়ে যাচ্ছে আর চোরদের ধরার কো...   \n",
       "2504  555021                        ফকিনি বাংলা দেশ কারেন্ট নাই   \n",
       "2505  858412  কাকু সামনে চশমা থাকবে কিন্তু গ্লাস থাকবেনাপাছা...   \n",
       "2509  200314         এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়   \n",
       "\n",
       "               label  toxic  label_id  \n",
       "0     Political Hate      1         3  \n",
       "1            Abusive      1         5  \n",
       "3            Abusive      1         5  \n",
       "4            Abusive      1         5  \n",
       "5            Abusive      1         5  \n",
       "...              ...    ...       ...  \n",
       "2496         Abusive      1         5  \n",
       "2503         Abusive      1         5  \n",
       "2504         Abusive      1         5  \n",
       "2505         Profane      1         4  \n",
       "2509         Abusive      1         5  \n",
       "\n",
       "[1061 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = train_df[train_df['toxic'] == 1].copy()\n",
    "\n",
    "df_val = dev_df[dev_df['toxic'] == 1].copy()\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cb5a4a-ba0d-42c1-9f6d-eabd510cfac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Categories: ['Abusive', 'Political Hate', 'Profane', 'Religious Hate', 'Sexism']\n"
     ]
    }
   ],
   "source": [
    "toxic_df = df_train[df_train['toxic'] == 1].copy()\n",
    "target_list = sorted(toxic_df['label'].unique().tolist()) # Sort for consistent column order\n",
    "print(f\"Target Categories: {target_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1d42a2-c1e2-4ab7-abc4-20332614797c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Abusive</th>\n",
       "      <th>Political Hate</th>\n",
       "      <th>Profane</th>\n",
       "      <th>Religious Hate</th>\n",
       "      <th>Sexism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>কি সাংঘাতিক ভাই রে তুই</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>লঞ্চ মালিকদের অভিশপ্ত চক্ষু পদ্মা সেতুর উপর</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>কিরে মানিক চোরা তুইও আছোস</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>দেশের সবই তো চুরি হয়ে যাচ্ছে আর চোরদের ধরার কো...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>ফকিনি বাংলা দেশ কারেন্ট নাই</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>কাকু সামনে চশমা থাকবে কিন্তু গ্লাস থাকবেনাপাছা...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1061 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  Abusive  \\\n",
       "0     ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...    False   \n",
       "1     লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...     True   \n",
       "3     আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...     True   \n",
       "4                                কি সাংঘাতিক ভাই রে তুই     True   \n",
       "5           লঞ্চ মালিকদের অভিশপ্ত চক্ষু পদ্মা সেতুর উপর     True   \n",
       "...                                                 ...      ...   \n",
       "2496                          কিরে মানিক চোরা তুইও আছোস     True   \n",
       "2503  দেশের সবই তো চুরি হয়ে যাচ্ছে আর চোরদের ধরার কো...     True   \n",
       "2504                        ফকিনি বাংলা দেশ কারেন্ট নাই     True   \n",
       "2505  কাকু সামনে চশমা থাকবে কিন্তু গ্লাস থাকবেনাপাছা...    False   \n",
       "2509         এই শালা ইবলিশ এর বস এবলিশ এদের দেখে ভয় পায়     True   \n",
       "\n",
       "      Political Hate  Profane  Religious Hate  Sexism  \n",
       "0               True    False           False   False  \n",
       "1              False    False           False   False  \n",
       "3              False    False           False   False  \n",
       "4              False    False           False   False  \n",
       "5              False    False           False   False  \n",
       "...              ...      ...             ...     ...  \n",
       "2496           False    False           False   False  \n",
       "2503           False    False           False   False  \n",
       "2504           False    False           False   False  \n",
       "2505           False     True           False   False  \n",
       "2509           False    False           False   False  \n",
       "\n",
       "[1061 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.get_dummies(df_train, columns=['label'], prefix='', prefix_sep='')[['text'] + target_list]\n",
    "\n",
    "df_val = pd.get_dummies(df_val, columns=['label'], prefix='', prefix_sep='')[['text'] + target_list]\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859019c4-8b71-43f6-b308-81452744e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        # Corrected column name from 'title' or 'Text' to 'text'\n",
    "        self.texts = list(df['text']) \n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN, target_list)\n",
    "val_dataset = CustomDataset(df_val, tokenizer, MAX_LEN, target_list)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c00cbc4-6678-4afc-9761-b917cde53c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.2005:  24%|██▍       | 237/973 [00:53<02:47,  4.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m     model, train_acc, train_loss \u001b[38;5;241m=\u001b[39m train_model(train_data_loader, model, optimizer)\n\u001b[1;32m    113\u001b[0m     val_acc, val_loss \u001b[38;5;241m=\u001b[39m eval_model(val_data_loader, model)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(training_loader, model, optimizer)\u001b[0m\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Accuracy calculation\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m outputs_sigmoid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround()\n\u001b[1;32m     69\u001b[0m targets_np \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     70\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(outputs_sigmoid \u001b[38;5;241m==\u001b[39m targets_np)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# class BERTClass(torch.nn.Module):\n",
    "#     def __init__(self, model_name, target_list):\n",
    "#         super(BERTClass, self).__init__()\n",
    "#         self.bert_model = AutoModel.from_pretrained(model_name, return_dict=True)\n",
    "#         self.dropout = torch.nn.Dropout(0.3)\n",
    "#         self.linear = torch.nn.Linear(self.bert_model.config.hidden_size, len(target_list))\n",
    "    \n",
    "#     def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "#         output = self.bert_model(\n",
    "#             input_ids, \n",
    "#             attention_mask=attn_mask, \n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "        \n",
    "#         # --- FIX IS HERE ---\n",
    "#         # Instead of pooler_output, we take the last hidden state of the [CLS] token\n",
    "#         # output.last_hidden_state has shape (batch_size, sequence_length, hidden_size)\n",
    "#         # We select the [CLS] token by indexing with [:, 0, :]\n",
    "#         cls_output = output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "#         output_dropout = self.dropout(cls_output)\n",
    "#         final_output = self.linear(output_dropout)\n",
    "        \n",
    "#         return final_output\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = BERTClass(MODEL_NAME, target_list)\n",
    "# model.to(device)\n",
    "\n",
    "# # ==================================\n",
    "# # 4. Loss Function and Optimizer\n",
    "# # ==================================\n",
    "# # For multi-label classification, BCEWithLogitsLoss is the correct choice.\n",
    "# # It combines a Sigmoid layer and the BCELoss in one single class.\n",
    "# def loss_fn(outputs, targets):\n",
    "#     return nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# # ==================================\n",
    "# # 5. Training and Evaluation Functions\n",
    "# # ==================================\n",
    "# # Your train_model and eval_model functions are well-written and can be used as they are.\n",
    "# # I've just adjusted the tqdm progress bar description for more clarity.\n",
    "\n",
    "# def train_model(training_loader, model, optimizer):\n",
    "#     model.train()\n",
    "#     losses = []\n",
    "#     correct_predictions = 0\n",
    "#     num_samples = 0\n",
    "    \n",
    "#     loop = tq(training_loader, leave=True)\n",
    "#     for batch_idx, data in enumerate(loop):\n",
    "#         ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "#         mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "#         token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "#         targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "#         outputs = model(ids, mask, token_type_ids)\n",
    "#         loss = loss_fn(outputs, targets)\n",
    "#         losses.append(loss.item())\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accuracy calculation\n",
    "#         outputs_sigmoid = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "#         targets_np = targets.cpu().detach().numpy()\n",
    "#         correct_predictions += np.sum(outputs_sigmoid == targets_np)\n",
    "#         num_samples += targets_np.size\n",
    "        \n",
    "#         loop.set_description(f\"Train - Loss: {loss.item():.4f}\")\n",
    "\n",
    "#     return model, float(correct_predictions) / num_samples, np.mean(losses)\n",
    "\n",
    "\n",
    "# def eval_model(validation_loader, model):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     correct_predictions = 0\n",
    "#     num_samples = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data in validation_loader:\n",
    "#             ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "#             mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "#             token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "#             targets = data['targets'].to(device, dtype=torch.float)\n",
    "            \n",
    "#             outputs = model(ids, mask, token_type_ids)\n",
    "#             loss = loss_fn(outputs, targets)\n",
    "#             losses.append(loss.item())\n",
    "\n",
    "#             outputs_sigmoid = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "#             targets_np = targets.cpu().detach().numpy()\n",
    "#             correct_predictions += np.sum(outputs_sigmoid == targets_np)\n",
    "#             num_samples += targets_np.size\n",
    "\n",
    "#     return float(correct_predictions) / num_samples, np.mean(losses)\n",
    "\n",
    "\n",
    "# # ==================================\n",
    "# # 6. Training Loop\n",
    "# # ==================================\n",
    "# history = defaultdict(list)\n",
    "# best_accuracy = 0\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     print(f'\\n--- Epoch {epoch}/{EPOCHS} ---')\n",
    "    \n",
    "#     model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "#     val_acc, val_loss = eval_model(val_data_loader, model)\n",
    "\n",
    "#     print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "#     print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "#     history['train_acc'].append(train_acc)\n",
    "#     history['train_loss'].append(train_loss)\n",
    "#     history['val_acc'].append(val_acc)\n",
    "#     history['val_loss'].append(val_loss)\n",
    "\n",
    "#     if val_acc > best_accuracy:\n",
    "#         torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "#         best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a2bdbba-df12-4b20-b1f8-b1c4f8767160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 1: Binary Toxicity Detector ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 1 - Epoch 1:   7%|▋         | 160/2221 [00:34<07:26,  4.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m ids, mask, token_ids, targets \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     75\u001b[0m optimizer_1\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_1(ids, mask, token_ids)\n\u001b[1;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn_1(outputs, targets)\n\u001b[1;32m     78\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36mBERTClass_Binary.forward\u001b[0;34m(self, input_ids, attn_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attn_mask, token_type_ids):\n\u001b[0;32m---> 52\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattn_mask, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[1;32m     53\u001b[0m     cls_output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     54\u001b[0m     output_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(cls_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/electra/modeling_electra.py:789\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_project\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    787\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_project(hidden_states)\n\u001b[0;32m--> 789\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    790\u001b[0m     hidden_states,\n\u001b[1;32m    791\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    792\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    793\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    794\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    795\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    796\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    797\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    798\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    799\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/electra/modeling_electra.py:559\u001b[0m, in \u001b[0;36mElectraEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    555\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    557\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    560\u001b[0m     hidden_states,\n\u001b[1;32m    561\u001b[0m     attention_mask,\n\u001b[1;32m    562\u001b[0m     layer_head_mask,\n\u001b[1;32m    563\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    565\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    566\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    567\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    568\u001b[0m )\n\u001b[1;32m    570\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/electra/modeling_electra.py:496\u001b[0m, in \u001b[0;36mElectraLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    493\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    494\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    498\u001b[0m )\n\u001b[1;32m    499\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pytorch_utils.py:226\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be a tuple/list of tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m num_args_in_forward_chunk_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(forward_fn)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_args_in_forward_chunk_fn \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_tensors):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_chunk_fn expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_args_in_forward_chunk_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors are given\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:3335\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3336\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:3075\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3073\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3075\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   3076\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3077\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:2517\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is not a callable object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj))\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mMethodType):\n\u001b[1;32m   2515\u001b[0m     \u001b[38;5;66;03m# In this case we skip the first parameter of the underlying\u001b[39;00m\n\u001b[1;32m   2516\u001b[0m     \u001b[38;5;66;03m# function (usually `self` or `cls`).\u001b[39;00m\n\u001b[0;32m-> 2517\u001b[0m     sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n\u001b[1;32m   2520\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _signature_bound_method(sig)\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:2587\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2588\u001b[0m                                     skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg,\n\u001b[1;32m   2589\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2593\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:2477\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2472\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(Parameter(name, annotation\u001b[38;5;241m=\u001b[39mannotation,\n\u001b[1;32m   2473\u001b[0m                                 kind\u001b[38;5;241m=\u001b[39m_VAR_KEYWORD))\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;66;03m# Is 'func' is a pure Python function - don't validate the\u001b[39;00m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;66;03m# parameters list (for correct order and defaults), it should be OK.\u001b[39;00m\n\u001b[0;32m-> 2477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(parameters,\n\u001b[1;32m   2478\u001b[0m            return_annotation\u001b[38;5;241m=\u001b[39mannotations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m, _empty),\n\u001b[1;32m   2479\u001b[0m            __validate_parameters__\u001b[38;5;241m=\u001b[39mis_duck_function)\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:3066\u001b[0m, in \u001b[0;36mSignature.__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m   3064\u001b[0m             params[name] \u001b[38;5;241m=\u001b[39m param\n\u001b[1;32m   3065\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3066\u001b[0m         params \u001b[38;5;241m=\u001b[39m OrderedDict((param\u001b[38;5;241m.\u001b[39mname, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMappingProxyType(params)\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_annotation \u001b[38;5;241m=\u001b[39m return_annotation\n",
      "File \u001b[0;32m/gpfs1/sw/rh9/pkgs/python3.12-anaconda/2024.06-1/lib/python3.12/inspect.py:3066\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3064\u001b[0m             params[name] \u001b[38;5;241m=\u001b[39m param\n\u001b[1;32m   3065\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3066\u001b[0m         params \u001b[38;5;241m=\u001b[39m OrderedDict((param\u001b[38;5;241m.\u001b[39mname, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMappingProxyType(params)\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_annotation \u001b[38;5;241m=\u001b[39m return_annotation\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME =\"csebuetnlp/banglabert\"\n",
    "\n",
    "# You can keep these as they are or tune them\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "#VAL_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# 4. Model 1: Binary Toxicity Detector\n",
    "# ==================================\n",
    "print(\"\\n--- Training Model 1: Binary Toxicity Detector ---\")\n",
    "\n",
    "class CustomDataset_Binary(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = list(df['text'])\n",
    "        self.targets = list(df['toxic'])\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len, padding='max_length',\n",
    "            return_token_type_ids=True, truncation=True, return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor([self.targets[index]])\n",
    "        }\n",
    "\n",
    "class BERTClass_Binary(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BERTClass_Binary, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(model_name, return_dict=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(self.bert_model.config.hidden_size, 1) # Output is 1 for binary\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        output_dropout = self.dropout(cls_output)\n",
    "        final_output = self.linear(output_dropout)\n",
    "        return final_output\n",
    "\n",
    "# Create DataLoaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_dataset_1 = CustomDataset_Binary(train_df, tokenizer, MAX_LEN)\n",
    "dev_dataset_1 = CustomDataset_Binary(dev_df, tokenizer, MAX_LEN)\n",
    "train_loader_1 = DataLoader(train_dataset_1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_1 = DataLoader(dev_dataset_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Instantiate Model, Loss, Optimizer\n",
    "model_1 = BERTClass_Binary(MODEL_NAME).to(device)\n",
    "loss_fn_1 = nn.BCEWithLogitsLoss()\n",
    "optimizer_1 = AdamW(model_1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training Loop (simplified for brevity)\n",
    "model_1.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in tq(train_loader_1, desc=f\"Model 1 - Epoch {epoch+1}\"):\n",
    "        ids, mask, token_ids, targets = data['input_ids'].to(device), data['attention_mask'].to(device), data['token_type_ids'].to(device), data['targets'].to(device)\n",
    "        optimizer_1.zero_grad()\n",
    "        outputs = model_1(ids, mask, token_ids)\n",
    "        loss = loss_fn_1(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "torch.save(model_1.state_dict(), \"model_1_binary_state.bin\")\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6dc4dee7-7eaf-4701-a639-cf5a47bcef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Pipeline on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Evaluating Pipeline on Test Set ---\")\n",
    "\n",
    "# Load the trained model weights\n",
    "\n",
    "model_1.load_state_dict(torch.load(\"model_1_binary_state.bin\"))\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model_state.bin\"))\n",
    "\n",
    "model_1.eval()\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "841bbe99-3fbf-43e7-b2bc-3447e8e72bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test data: 100%|██████████| 2512/2512 [00:22<00:00, 111.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import  tqdm\n",
    "\n",
    "def predict_toxicity_pipeline(text, tokenizer, model_1, model_2, device, max_len, target_list):\n",
    "    # --- Tokenization (no changes here) ---\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, add_special_tokens=True, max_length=max_len, padding='max_length',\n",
    "        return_token_type_ids=True, truncation=True, return_attention_mask=True, return_tensors='pt'\n",
    "    )\n",
    "    ids = inputs['input_ids'].to(device)\n",
    "    mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Check if 'token_type_ids' exists in the tokenizer output\n",
    "    token_ids = inputs['token_type_ids'].to(device) if 'token_type_ids' in inputs else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Step 1: Check if toxic with Model 1\n",
    "        # The model forward pass may vary depending on its definition\n",
    "        output_1 = model_1(ids, mask, token_ids)\n",
    "        # Assuming the output is logits from a SequenceClassifierOutput object\n",
    "        prob_1 = torch.sigmoid(output_1).item()\n",
    "\n",
    "        if prob_1 < 0.5:\n",
    "            # Not toxic, return a vector of all zeros\n",
    "            return np.zeros(len(target_list), dtype=int)\n",
    "        else:\n",
    "            # Toxic, proceed to Model 2\n",
    "            output_2 =  model_2(ids, mask, token_ids)\n",
    "            \n",
    "            # --- 👇 KEY CHANGE IS HERE ---\n",
    "            # 1. Find the index of the label with the highest score (logit)\n",
    "            # We use argmax directly on the logits, which is efficient.\n",
    "            # --- AFTER ---\n",
    "            pred_index = torch.argmax(output_2, dim=1).item()\n",
    "            \n",
    "            # 2. Create a one-hot encoded vector\n",
    "            # This creates an array of zeros...\n",
    "            one_hot_prediction = np.zeros(len(target_list), dtype=int)\n",
    "            # ...and sets the predicted index to 1.\n",
    "            one_hot_prediction[pred_index] = 1\n",
    "            \n",
    "            return one_hot_prediction\n",
    "\n",
    "# --- Your evaluation loop (assuming target_list is defined) ---\n",
    "# Example: target_list = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "all_predictions = []\n",
    "# Ensure test_df, tokenizer, model_1, etc. are correctly defined and loaded\n",
    "for text in tq(test_df['text'], desc=\"Predicting on test data\"):\n",
    "    prediction = predict_toxicity_pipeline(text, tokenizer, model_1, model, device, MAX_LEN, target_list)\n",
    "    all_predictions.append(prediction)\n",
    "\n",
    "y_pred = np.array(all_predictions)\n",
    "\n",
    "# Now y_pred will be a 2D array where each row has at most one '1'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e730fbf-d481-4f77-bde6-a15d26066e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43f05743-943a-40cc-8b1a-9d9a60fe755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_Label\n",
      "0            None\n",
      "1         Profane\n",
      "2            None\n",
      "3            None\n",
      "4            None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#'Abusive', 'Political Hate', 'Profane', 'Religious Hate', 'Sexism']\n",
    "\n",
    "# Your mapping\n",
    "id2l = {\n",
    "    0: 'Abusive',\n",
    "    1: 'Political Hate',\n",
    "    2: 'Profane',\n",
    "    3:  'Religious Hate',\n",
    "    4: 'Sexism'\n",
    "}\n",
    "\n",
    "# Example y_pred\n",
    "# y_pred = np.array([[0,0,0,0,0],[0,0,0,1,0],[1,0,0,0,0]])\n",
    "\n",
    "def decode_labels(row):\n",
    "    indices = np.where(row == 1)[0]\n",
    "    if len(indices) == 0:\n",
    "        return \"None\"\n",
    "    # If multiple labels, join them with comma\n",
    "    return \", \".join([id2l[i] for i in indices])\n",
    "\n",
    "# Convert predictions into a DataFrame column\n",
    "df = pd.DataFrame()\n",
    "df[\"Predicted_Label\"] = [decode_labels(row) for row in y_pred]\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c4c6efe-b4cb-4c39-b175-9179574eab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>শুভ কামনা রইল বাংলাদেশ জন্য ইনশাআল্লাহ জয় হবে</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>গোয়া মারা দিয়ে আছে বাংলাদেশ মাদারচোদ নিউজ করে ...</td>\n",
       "      <td>Profane</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>ভাইয়া আপনি অভিনেতা হইয়েন না না হলে সবাই বাচ্...</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>আমাদেরো তাই দেখছি</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>পুলিশ কতটা টাকা নিয়ে</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>সত্য কথা তেতু লাগে</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>এই ফকিননি মাগীটা আর কত নাটক দেখাবে</td>\n",
       "      <td>Profane</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>দেখো আজকে কার ফিটনেস কোথায় দাঁড়িয়েছে তুমি চ...</td>\n",
       "      <td>Profane</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>ছোট ভাইটির পাসে থাকুন গেম ভিড়িও বানাই</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>মরক্কোর জন্য শুভকামনা</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text    label  \\\n",
       "0     879187     শুভ কামনা রইল বাংলাদেশ জন্য ইনশাআল্লাহ জয় হবে     None   \n",
       "1     316919  গোয়া মারা দিয়ে আছে বাংলাদেশ মাদারচোদ নিউজ করে ...  Profane   \n",
       "2     916242  ভাইয়া আপনি অভিনেতা হইয়েন না না হলে সবাই বাচ্...     None   \n",
       "3     786824                                  আমাদেরো তাই দেখছি     None   \n",
       "4      47284                               পুলিশ কতটা টাকা নিয়ে     None   \n",
       "...      ...                                                ...      ...   \n",
       "2507  776466                                 সত্য কথা তেতু লাগে     None   \n",
       "2508  849227                 এই ফকিননি মাগীটা আর কত নাটক দেখাবে  Profane   \n",
       "2509  532697  দেখো আজকে কার ফিটনেস কোথায় দাঁড়িয়েছে তুমি চ...  Profane   \n",
       "2510  861411              ছোট ভাইটির পাসে থাকুন গেম ভিড়িও বানাই     None   \n",
       "2511  743242                              মরক্কোর জন্য শুভকামনা     None   \n",
       "\n",
       "            model  \n",
       "0     bangla-bert  \n",
       "1     bangla-bert  \n",
       "2     bangla-bert  \n",
       "3     bangla-bert  \n",
       "4     bangla-bert  \n",
       "...           ...  \n",
       "2507  bangla-bert  \n",
       "2508  bangla-bert  \n",
       "2509  bangla-bert  \n",
       "2510  bangla-bert  \n",
       "2511  bangla-bert  \n",
       "\n",
       "[2512 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label']=df['Predicted_Label']\n",
    "test_df['model']='bangla-bert'\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c60afaa-aa19-4f5d-a2d7-b73ddfb401e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879187</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316919</td>\n",
       "      <td>Profane</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916242</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786824</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47284</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>776466</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>849227</td>\n",
       "      <td>Profane</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>532697</td>\n",
       "      <td>Profane</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>861411</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>743242</td>\n",
       "      <td>None</td>\n",
       "      <td>bangla-bert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    label        model\n",
       "0     879187     None  bangla-bert\n",
       "1     316919  Profane  bangla-bert\n",
       "2     916242     None  bangla-bert\n",
       "3     786824     None  bangla-bert\n",
       "4      47284     None  bangla-bert\n",
       "...      ...      ...          ...\n",
       "2507  776466     None  bangla-bert\n",
       "2508  849227  Profane  bangla-bert\n",
       "2509  532697  Profane  bangla-bert\n",
       "2510  861411     None  bangla-bert\n",
       "2511  743242     None  bangla-bert\n",
       "\n",
       "[2512 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df[['id', 'label', 'model']]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "085e9ba6-75ea-44cb-8375-0b69f67e203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to final_ensemble.tsv\n"
     ]
    }
   ],
   "source": [
    "test_df.to_csv(\"final_banth_v7.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Saved to final_ensemble.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec92b21-0267-482a-b8c2-11f9cf591d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
